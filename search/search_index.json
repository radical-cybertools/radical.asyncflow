{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#radical-asyncflow-raf","title":"RADICAL AsyncFlow (RAF)","text":"<p>RADICAL AsyncFlow (RAF) is a fast asynchronous scripting library built on top of asyncio for building powerful asynchronous workflows on HPC, clusters, and local machines. It supports pluggable execution backends with intuitive task dependencies and workflow composition. </p> <ul> <li> <p>\u26a1 Powerful asynchronous workflows \u2014 Compose complex async and sync workflows easily, with intuitive task dependencies.</p> </li> <li> <p>\ud83c\udf10 Portable across environments \u2014 Run seamlessly on HPC systems, clusters, and local machines with pluggable execution backends.</p> </li> <li> <p>\ud83e\udde9 Flexible and extensible \u2014 Supports composite workflows management.</p> </li> </ul> <p>Currently, RAF supports the following execution backends:</p> <ul> <li>Radical.Pilot</li> <li>Dask.Parallel</li> <li>Concurrent.Executor</li> <li>Noop with <code>dry_run</code></li> <li>Custom implementations</li> </ul>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom radical.asyncflow import WorkflowEngine\nfrom radical.asyncflow import ConcurrentExecutionBackend\n\nasync def run():\n    # Create backend and workflow\n    backend = await ConcurrentExecutionBackend(ThreadPoolExecutor(max_workers=3))\n    flow = await WorkflowEngine.create(backend=backend)\n\n    @flow.executable_task\n    async def task1():\n        return \"echo $RANDOM\"\n\n    @flow.function_task\n    async def task2(t1_result):\n        return int(t1_result.strip()) * 2 * 2\n\n    # create the workflow\n    t1_fut = task1()\n    t2_result = await task2(t1_fut) # t2 depends on t1 (waits for it)\n\n    # shutdown the execution backend\n    await flow.shutdown()\n\nif __name__ == \"__main__\":\n    asyncio.run(run())\n</code></pre>"},{"location":"async_workflows/","title":"Asynchronous Workflows","text":""},{"location":"async_workflows/#synchronous-vs-asynchronous-programming-with-asyncflow","title":"Synchronous vs Asynchronous Programming with AsyncFlow","text":"<p>The goal of this documentation is to demonstrate the power of asynchronous programming enabled by AsyncFlow for both blocking and non-blocking tasks, workflows, and workflow compositions.</p>"},{"location":"async_workflows/#building-asynchronous-workflows","title":"Building Asynchronous Workflows","text":"<p>In the asynchronous approach, we submit all 5 blocking workflows concurrently and wait for their completion.</p> <pre><code>graph TD\n    A[Start] --&gt; B[Launch N workflows async]\n    B --&gt; C1[Workflow 1]\n    B --&gt; C2[Workflow 2]\n    B --&gt; C3[Workflow ...]\n    B --&gt; C4[Workflow N]\n    C1 --&gt; D[Wait for all to finish]\n    C2 --&gt; D\n    C3 --&gt; D\n    C4 --&gt; D\n    D --&gt; E[Print total time]\n    E --&gt; F[Shutdown WorkflowEngine]</code></pre> <p>Performance Benefit</p> <p>This approach can significantly reduce total execution time by allowing independent workflows to run concurrently.</p>"},{"location":"async_workflows/#example-code","title":"Example Code","text":"<pre><code>import time\nimport asyncio\nfrom radical.asyncflow import ConcurrentExecutionBackend\nfrom concurrent.futures import ThreadPoolExecutor\nfrom radical.asyncflow import WorkflowEngine\n\nbackend = await ConcurrentExecutionBackend(ThreadPoolExecutor())\nflow = await WorkflowEngine.create(backend=backend)\n\nasync def main():\n    @flow.function_task\n    async def task1(*args):\n        return time.time()\n\n    @flow.function_task\n    async def task2(*args):\n        return time.time()\n\n    @flow.function_task\n    async def task3(*args):\n        return time.time()\n\n    async def run_wf(wf_id):\n        print(f'Starting workflow {wf_id} at {time.time()}')\n        t3 = task3(task1(), task2())\n        await t3 # Blocking operation so the entire workflow will block\n        print(f'Workflow {wf_id} completed at {time.time()}')\n\n    start_time = time.time()\n    await asyncio.gather(*[run_wf(i) for i in range(5)])\n    end_time = time.time()\n\n    print(f'\\nTotal time running asynchronously is: {end_time - start_time}')\n\n    # We are in an async context, so we have to use await\n    await flow.shutdown()\n\nasyncio.run(main())\n</code></pre> Workflow log <pre><code>ThreadPool execution backend started successfully\nStarting workflow 0 at 1752767251.5312994\nStarting workflow 1 at 1752767251.5316885\nStarting workflow 2 at 1752767251.5318878\nStarting workflow 3 at 1752767251.532685\nStarting workflow 4 at 1752767251.5327375\nWorkflow 2 completed at 1752767251.5644567\nWorkflow 0 completed at 1752767251.564515\nWorkflow 1 completed at 1752767251.5645394\nWorkflow 4 completed at 1752767251.5645616\nWorkflow 3 completed at 1752767251.5645802\n\nTotal time running asynchronously is: 0.03412771224975586\nShutdown is triggered, terminating the resources gracefully\n</code></pre> <p>Key Characteristics</p> <ul> <li>Workflows execute concurrently</li> <li>Total time is determined by the longest-running workflow</li> <li>More efficient but requires proper async/await syntax</li> <li>Better resource utilization</li> </ul> <p>When to Use Each</p> <ul> <li>Use synchronous when workflows must run in sequence or have dependencies</li> <li>Use asynchronous when workflows are independent and you want better performance</li> </ul>"},{"location":"basic/","title":"Basic Usage","text":""},{"location":"basic/#getting-started-run-a-single-workflow-with-task-dependencies","title":"Getting Started: Run a Single Workflow with Task Dependencies","text":"<p>This guide walks you through running a single workflow with task dependencies using <code>radical.asyncflow</code>.</p> <p>You\u2019ll learn how to define tasks, set dependencies, execute the workflow, and shut down the engine gracefully.</p>"},{"location":"basic/#prerequisites","title":"Prerequisites","text":"<ul> <li>Make sure you have installed <code>radical.asyncflow</code> in your Python environment.  </li> <li>You also need a working Jupyter Notebook or Python &gt;=3.8.</li> </ul>"},{"location":"basic/#import-the-necessary-modules","title":"Import the necessary modules","text":"<p>You\u2019ll need <code>time</code>, <code>asyncio</code>, and the key classes from <code>radical.asyncflow</code>.</p> <pre><code>import time\nimport asyncio\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom radical.asyncflow import WorkflowEngine, ConcurrentExecutionBackend\n</code></pre>"},{"location":"basic/#set-up-the-workflow-engine","title":"Set up the workflow engine","text":"<p>We initialize the workflow engine with a <code>ConcurrentExecutionBackend</code> using Python\u2019s <code>ThreadPoolExecutor</code> or <code>ProcessPoolExecutor</code>.</p> <pre><code>backend = await ConcurrentExecutionBackend(ThreadPoolExecutor())\nflow = await WorkflowEngine.create(backend=backend)\n</code></pre>"},{"location":"basic/#define-the-tasks","title":"Define the tasks","text":"<p>Define your tasks and specify dependencies by passing other tasks as arguments.</p> <pre><code>@flow.function_task\nasync def task1(*args):\n    print(f\"Running task1 at {time.time()}\")\n    await asyncio.sleep(1)\n    return time.time()\n\n@flow.function_task\nasync def task2(*args):\n    print(f\"Running task2 at {time.time()}\")\n    await asyncio.sleep(1)\n    return time.time()\n\n@flow.function_task\nasync def task3(t1_result, t2_result):\n    print(f\"Running task3 after task1 and task2 at {time.time()}\")\n    await asyncio.sleep(1)\n    return time.time()\n</code></pre> <p>Note</p> <ul> <li><code>task3</code> depends on the outputs of <code>task1</code> and <code>task2</code>.</li> <li>You express this dependency by calling <code>task3(task1(), task2())</code>.</li> <li><code>task1</code> and <code>task2</code> will be automatically resolved during runtime and their values will be assigned to <code>task3</code> accordingly.</li> </ul>"},{"location":"basic/#run-the-workflow","title":"Run the workflow","text":"<p>We now submit the tasks with their dependencies and wait for the final task to complete.</p> <pre><code>async def main():\n    start_time = time.time()\n\n    # Define the task graph\n    t1 = task1()\n    t2 = task2()\n    t3 = task3(t1, t2)\n\n    # Wait for the final task to complete\n    await t3\n\n    end_time = time.time()\n    print(f\"\\nWorkflow completed in: {end_time - start_time:.2f} seconds\")\n\n    # Shutdown the workflow engine\n    await flow.shutdown()\n\nasyncio.run(main())\n</code></pre>"},{"location":"basic/#example-output","title":"Example Output","text":"<p>Here\u2019s an example of the output you might see:</p> Show Output Log <pre><code>Running task1 at 1721847632.3501\nRunning task2 at 1721847632.3505\nRunning task3 after task1 and task2 at 1721847633.3534\n\nWorkflow completed in: 0.02 seconds\n</code></pre> <p>Warning</p> <p>Make sure to await the shutdown of the <code>WorkflowEngine</code> before your script exits. Otherwise, resources may leak.</p>"},{"location":"basic/#summary","title":"Summary","text":"<p>You now know how to:</p> <ul> <li>Define a set of tasks with dependencies.  </li> <li>Submit them to the workflow engine.  </li> <li>Run the workflow asynchronously.  </li> <li>Shut down the engine properly.</li> </ul>"},{"location":"basic/#next-steps","title":"Next Steps","text":"<ul> <li>Learn how to run multiple workflows concurrently.</li> <li>Explore advanced backends like <code>DaskExecutionBackend</code> or <code>RadicalExecutionBackend</code>.</li> <li>Integrate with HPC schedulers.</li> </ul>"},{"location":"best_practice/","title":"Best Practices","text":""},{"location":"best_practice/#best-practices-for-asyncflow","title":"Best Practices for AsyncFlow","text":"<p>AsyncFlow is built on top of Python\u2019s <code>asyncio</code>, combining asynchronous execution and task dependency management with a simple API. This page outlines recommended practices when using AsyncFlow effectively in your projects.</p>"},{"location":"best_practice/#why-best-practices-matter","title":"Why Best Practices Matter","text":"<p>Async programming can easily lead to:</p> <ul> <li>Hard-to-debug concurrency bugs \ud83d\udea8</li> <li>Deadlocks or race conditions \ud83d\udca5</li> <li>Inefficient task scheduling \u26a0\ufe0f</li> </ul> <p>By following these best practices, you can:</p> <ul> <li>Make your workflows reliable \ud83c\udfc5</li> <li>Maximize concurrency \ud83d\udd25</li> <li>Keep code maintainable \ud83d\udd27</li> </ul>"},{"location":"best_practice/#structure-your-tasks-clearly","title":"Structure Your Tasks Clearly","text":"<ul> <li>Define small, composable tasks that each do one thing well.</li> <li>Prefer pure functions or side-effect-free coroutines as tasks.</li> <li>Use <code>@flow.function_task</code> or <code>@flow.executable_task</code> decorators consistently.</li> </ul> <p>Tip</p> <p>Name your tasks clearly to improve logs and debugging </p><pre><code>@flow.function_task\nasync def fetch_data(url):\n    ...\n</code></pre><p></p>"},{"location":"best_practice/#use-dependencies-correctly","title":"Use Dependencies Correctly","text":"<p>Tasks can depend on the output of other tasks: - Make your dependency graph explicit by passing tasks as arguments. - Don\u2019t block unnecessarily: let AsyncFlow schedule dependencies.</p> <p>Tip</p> <p>Tasks that don\u2019t depend on each other run in parallel automatically.</p> <pre><code>asyncflow = await WorkflowManager.create(dry_run=True)\n\nasync def task_a():\n    asyncio.sleep(2) # (1)!\n\nasync def task_a():\n    asyncio.sleep(2) # (2)!\n\nasync def task_c(task_a_fut, task_b_fut):\n    asyncio.sleep(2)\n\n\nresult = task_c(task_a(), task_b()) # (3)!\n</code></pre> <ol> <li>Task A will run asynchronously (independently)</li> <li>Task B will run asynchronously (independently)</li> <li>Task C will wait implicitly for other tasks</li> </ol>"},{"location":"best_practice/#await-only-at-the-top-level","title":"Await Only at the Top Level","text":"<ul> <li>Inside your workflow logic, don\u2019t await intermediate tasks.</li> <li>Let AsyncFlow build the graph; only await the final or root tasks you care about.</li> <li>Awaiting early forces serialization and kills concurrency.</li> </ul> <p>Warning</p> <p>Avoid this as it will be slower:</p> <p></p><pre><code>await task_a()\nawait task_b()\n</code></pre> Instead 1 await is faster \ud83d\udd25:<p></p> <pre><code>result = await task_c(task_a(), task_b())\n</code></pre>"},{"location":"best_practice/#use-await-flowshutdown","title":"Use <code>await flow.shutdown()</code>","text":"<p>Always shut down the flow explicitly when finished: - Releases resources (e.g., thread pools, processes). - Ensures a clean exit.</p> <p>At the end of your async main:</p> <pre><code>await flow.shutdown()\n</code></pre>"},{"location":"best_practice/#logging-debugging","title":"Logging &amp; Debugging","text":"<p>Enable detailed logs to diagnose issues: </p><pre><code>export RADICAL_LOG_LVL=DEBUG\n</code></pre><p></p> <p>Logs show task dependencies, execution order, errors.</p>"},{"location":"best_practice/#clean-shutdown","title":"Clean Shutdown","text":"<ul> <li>Use <code>try</code>/<code>finally</code> in your async main to ensure <code>flow.shutdown()</code> is always called, even on exceptions.</li> </ul> <p>Success</p> <ul> <li>Define tasks clearly and concisely.  </li> <li>Pass tasks as arguments to express dependencies.  </li> <li>Only await at the top level.  </li> <li>Shut down cleanly.  </li> <li>Log at <code>DEBUG</code> level when needed.  </li> </ul>"},{"location":"composite_workflow/","title":"Composite Workflows","text":""},{"location":"composite_workflow/#composite-workflows","title":"Composite Workflows","text":"<p>AsyncFlow provides capabilities to define, construct, and combine multiple workflows into a single composite workflow. We refer to these composite workflow(s) as <code>Block(s)</code> - a logical grouping of dependent and independent workflows.</p> <p>This page walks you step by step through defining and running composite workflows in AsyncFlow.</p> <pre><code>graph TD\n\n    subgraph Block A\n        A_WF1[task1 --&gt; task2 --&gt; task3] --&gt; A_WF2[task1 --&gt; task2 --&gt; task3] --&gt; A_WF3[task1 --&gt; task2 --&gt; task3] \n    end\n\n    subgraph Block B\n        B_WF1[task1 --&gt; task2 --&gt; task3] --&gt; B_WF2[task1 --&gt; task2 --&gt; task3] --&gt; B_WF3[task1 --&gt; task2 --&gt; task3] \n    end\n\n    subgraph Block C\n        C_WF1[task1 --&gt; task2 --&gt; task3] --&gt; C_WF2[task1 --&gt; task2 --&gt; task3] --&gt; C_WF3[task1 --&gt; task2 --&gt; task3] \n    end\n</code></pre> <p>Note</p> <p><code>Block</code> entity can have DAG shaped workflows where some workflows depends on others. </p>"},{"location":"composite_workflow/#example-independent-blocks","title":"Example: Independent Blocks","text":"<p>Below is a full working example using <code>ConcurrentExecutionBackend</code> and Python's asyncio to execute three blocks in parallel, each with four dependent steps.</p>"},{"location":"composite_workflow/#setup","title":"Setup","text":"<pre><code>import time\nimport asyncio\nfrom radical.asyncflow import ConcurrentExecutionBackend\nfrom radical.asyncflow import WorkflowEngine\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nbackend = await ConcurrentExecutionBackend(ThreadPoolExecutor())\nasyncflow = await WorkflowEngine.create(backend=backend)\n</code></pre>"},{"location":"composite_workflow/#define-tasks","title":"Define Tasks","text":"<p>We now define 3 reusable tasks and a block representing one composite workflow.</p> <pre><code>@asyncflow.function_task\nasync def task1(name: str):\n    now = time.time()\n    print(f\"[{now:.2f}] {name} started\")\n    await asyncio.sleep(0.5)  # simulate work\n    print(f\"[{time.time():.2f}] {name} completed\")\n    return now\n\n@asyncflow.function_task\nasync def task2(name: str):\n    now = time.time()\n    print(f\"[{now:.2f}] {name} started\")\n    await asyncio.sleep(0.5)  # simulate work\n    print(f\"[{time.time():.2f}] {name} completed\")\n    return now\n\n@asyncflow.function_task\nasync def task3(name: str, *args):\n    now = time.time()\n    print(f\"[{now:.2f}] {name} started\")\n    await asyncio.sleep(0.5)  # simulate work\n    print(f\"[{time.time():.2f}] {name} completed\")\n    return now\n</code></pre> <pre><code>async def create_workflow(name):\n    now = time.time()\n    print(f\"[{now:.2f}] {name} started\")\n    t1 = task1('task1')\n    t2 = task2('task2')\n    t3 = task3('task3', t1, t2)\n    await t3\n    print(f\"[{time.time():.2f}] {name} completed\")\n</code></pre> <p>Success</p> <p><code>@asyncflow.function_task</code> turns a regular <code>async</code> function into an <code>AsyncFlow</code> task, which can be tracked, scheduled, and executed by the workflow engine.</p>"},{"location":"composite_workflow/#define-a-composite-workflow-block","title":"Define a Composite Workflow Block","text":"<pre><code>@asyncflow.block # (1)!\nasync def create_block(name: str, *args):\n    wf1 = await create_workflow('WF1')\n    wf2 = await create_workflow('WF2')\n    wf3 = await create_workflow('WF3')\n    print(f\"Processing {name} completed at {time.time():.2f}\")\n</code></pre> <ol> <li>Define one composite workflow block containing multiple dependent steps</li> </ol> <p>Tip</p> <p>The <code>@asyncflow.block</code> decorator groups a set of dependent tasks into a single logical unit (a \"block\"). This creates a composite workflow that can be executed dependently or independently from other blocks.</p>"},{"location":"composite_workflow/#run-all-blocks-concurrently","title":"Run All Blocks Concurrently","text":"<pre><code>start_time = time.time()\n\nawait asyncio.gather( # (1)!\n    create_block(\"Block A\"),\n    create_block(\"Block B\"),\n    create_block(\"Block C\"),\n)\n\nend_time = time.time()\nprint(f\"\\nTotal time running asynchronously is: {end_time - start_time:.2f}s\")\n\nawait asyncflow.shutdown() # (2)!\n</code></pre> <ol> <li>Run all composite workflow blocks concurrently</li> <li>Shutdown the workflow engine and terminate the resources</li> </ol> Execution log <pre><code>Concurrent execution backend started successfully\n[1753116467.34] WF1 started\n[1753116467.34] WF1 started\n[1753116467.34] WF1 started\n[1753116467.35] task1 started\n[1753116467.35] task2 started\n[1753116467.35] task1 started\n[1753116467.35] task2 started\n[1753116467.85] task1 completed\n[1753116467.85] task2 completed\n[1753116467.85] task1 completed\n[1753116467.85] task2 completed\n[1753116467.86] task1 started\n[1753116467.86] task2 started\n[1753116468.36] task1 completed\n[1753116468.36] task2 completed\n[1753116468.86] task3 started\n[1753116468.86] task3 started\n[1753116468.86] task3 started\n[1753116469.36] task3 completed\n[1753116469.36] task3 completed\n[1753116469.36] task3 completed\n[1753116469.38] WF1 completed\n[1753116469.38] WF2 started\n[1753116469.38] WF1 completed\n[1753116469.38] WF2 started\n[1753116469.38] WF1 completed\n[1753116469.38] WF2 started\n[1753116469.39] task2 started\n[1753116469.39] task1 started\n[1753116469.39] task1 started\n[1753116469.39] task2 started\n[1753116469.89] task1 completed\n[1753116469.89] task2 completed\n[1753116469.89] task1 completed\n[1753116469.89] task1 started\n[1753116469.89] task2 completed\n[1753116469.89] task2 started\n[1753116470.39] task1 completed\n[1753116470.39] task2 completed\n[1753116470.89] task3 started\n[1753116470.89] task3 started\n[1753116470.89] task3 started\n[1753116471.39] task3 completed\n[1753116471.39] task3 completed\n[1753116471.39] task3 completed\n[1753116471.42] WF2 completed\n[1753116471.42] WF3 started\n[1753116471.42] WF2 completed\n[1753116471.42] WF3 started\n[1753116471.42] WF2 completed\n[1753116471.42] WF3 started\n[1753116471.42] task1 started\n[1753116471.42] task1 started\n[1753116471.42] task2 started\n[1753116471.42] task2 started\n[1753116471.92] task1 completed\n[1753116471.92] task1 completed\n[1753116471.93] task2 completed\n[1753116471.93] task2 completed\n[1753116471.93] task1 started\n[1753116471.93] task2 started\n[1753116472.43] task2 completed\n[1753116472.43] task1 completed\n[1753116472.92] task3 started\n[1753116472.93] task3 started\n[1753116472.93] task3 started\n[1753116473.43] task3 completed\n[1753116473.43] task3 completed\n[1753116473.43] task3 completed\n[1753116473.45] WF3 completed\nProcessing Block C completed at 1753116473.45\n[1753116473.45] WF3 completed\nProcessing Block A completed at 1753116473.45\n[1753116473.45] WF3 completed\nProcessing Block B completed at 1753116473.45\n\nTotal time running asynchronously is: 6.12s\nShutdown is triggered, terminating the resources gracefully\n</code></pre> <p>Note</p> <ul> <li>Each block executes its steps sequentially.</li> <li>All blocks run concurrently.</li> <li>AsyncFlow handles scheduling and dependencies automatically.</li> </ul>"},{"location":"composite_workflow/#example-blocks-with-dependency","title":"Example: Blocks with Dependency","text":"<p>To represent the previous example as a <code>DAG</code> then all you need to do is to pass the handler (future) of each the dependent block to the depended block as follows:</p> <pre><code>block1 = create_block(\"Block A\") # (1)!\nblock2 = create_block(\"Block B\") # (2)!\nblock3 = create_block(\"Block C\", block1, block2) # (3)!\n\nawait block3\n</code></pre> <ol> <li><code>block1</code> will execute first without any waiting.</li> <li><code>block2</code> will execute at the same time of  <code>block1</code> without any waiting (in parallel).</li> <li><code>block3</code> will run only after <code>block1</code> and <code>block2</code> finishes execution successfully.</li> </ol> Execution log <pre><code>Concurrent execution backend started successfully\n[1753116817.42] WF1 started\n[1753116817.42] WF1 started\n[1753116817.43] task1 started\n[1753116817.43] task2 started\n[1753116817.43] task1 started\n[1753116817.43] task2 started\n[1753116817.93] task1 completed\n[1753116817.93] task2 completed\n[1753116817.93] task2 completed\n[1753116817.93] task1 completed\n[1753116818.93] task3 started\n[1753116818.93] task3 started\n[1753116819.43] task3 completed\n[1753116819.43] task3 completed\n[1753116819.46] WF1 completed\n[1753116819.46] WF2 started\n[1753116819.46] WF1 completed\n[1753116819.46] WF2 started\n[1753116819.47] task2 started\n[1753116819.47] task1 started\n[1753116819.47] task2 started\n[1753116819.47] task1 started\n[1753116819.97] task2 completed\n[1753116819.97] task1 completed\n[1753116819.97] task2 completed\n[1753116819.97] task1 completed\n[1753116820.97] task3 started\n[1753116820.97] task3 started\n[1753116821.47] task3 completed\n[1753116821.47] task3 completed\n[1753116821.50] WF2 completed\n[1753116821.50] WF3 started\n[1753116821.50] WF2 completed\n[1753116821.50] WF3 started\n[1753116821.50] task1 started\n[1753116821.50] task2 started\n[1753116821.50] task1 started\n[1753116821.50] task2 started\n[1753116822.00] task1 completed\n[1753116822.00] task2 completed\n[1753116822.00] task1 completed\n[1753116822.00] task2 completed\n[1753116823.00] task3 started\n[1753116823.01] task3 started\n[1753116823.51] task3 completed\n[1753116823.51] task3 completed\n[1753116823.53] WF3 completed\nProcessing Block B completed at 1753116823.53\n[1753116823.53] WF3 completed\nProcessing Block A completed at 1753116823.53\n[1753116823.55] WF1 started\n[1753116823.56] task1 started\n[1753116823.56] task2 started\n[1753116824.06] task2 completed\n[1753116824.06] task1 completed\n[1753116825.06] task3 started\n[1753116825.56] task3 completed\n[1753116825.59] WF1 completed\n[1753116825.59] WF2 started\n[1753116825.59] task1 started\n[1753116825.59] task2 started\n[1753116826.10] task1 completed\n[1753116826.10] task2 completed\n[1753116827.10] task3 started\n[1753116827.60] task3 completed\n[1753116827.63] WF2 completed\n[1753116827.63] WF3 started\n[1753116827.63] task1 started\n[1753116827.63] task2 started\n[1753116828.13] task1 completed\n[1753116828.13] task2 completed\n[1753116829.13] task3 started\n[1753116829.63] task3 completed\n[1753116829.66] WF3 completed\nProcessing Block C completed at 1753116829.66\n\nTotal time running asynchronously is: 12.25s\nShutdown is triggered, terminating the resources gracefully\n</code></pre> <p>Warning</p> <p>Do not forget to <code>await asyncflow.shutdown()</code> when you are done \u2014 otherwise, resources may remain allocated.</p> <p>Tip</p> <p>You can replace <code>ConcurrentExecutionBackend</code> with <code>RadicalExecutionBackend</code> if you want to run on an HPC cluster instead of local threads/processes.</p>"},{"location":"exec_backends/","title":"Execution Backends","text":""},{"location":"exec_backends/#execution-backends-seamless-backend-switching","title":"Execution Backends: Seamless Backend Switching","text":"<p>AsyncFlow's architecture follows a separation of concerns principle, completely isolating the execution backend from the asynchronous programming layer. This plug-and-play (PnP) design allows you to switch between different execution environments with minimal code changes \u2014 from local development to massive HPC clusters.</p>"},{"location":"exec_backends/#the-power-of-backend-abstraction","title":"The Power of Backend Abstraction","text":"<p>By design, AsyncFlow enforces that the execution backend should be entirely isolated from the asynchronous programming layer. This means you can seamlessly transition your workflows from:</p> <ul> <li>Local development with thread pools</li> <li>HPC clusters with thousands of nodes</li> <li>GPU clusters for accelerated computing</li> </ul> <p>The best part? Changing execution backends requires modifying just one line of code.</p>"},{"location":"exec_backends/#local-vs-hpc-execution-a-side-by-side-comparison","title":"Local vs HPC Execution: A Side-by-Side Comparison","text":""},{"location":"exec_backends/#local-execution-with-concurrentexecutionbackend","title":"Local Execution with ConcurrentExecutionBackend","text":"<pre><code># Local execution with threads\n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom radical.asyncflow import ConcurrentExecutionBackend\n\nbackend = ConcurrentExecutionBackend(ThreadPoolExecutor())\n</code></pre>"},{"location":"exec_backends/#hpc-execution-with-radicalexecutionbackend","title":"HPC Execution with RadicalExecutionBackend","text":"<pre><code># HPC execution with Radical.Pilot\nfrom radical.asyncflow import RadicalExecutionBackend\n\nbackend = RadicalExecutionBackend({'resource': 'local.localhost'})\n</code></pre> <p>Success</p> <p>One line change transforms your workflow from local thread execution to distributed HPC execution across thousands of nodes.</p>"},{"location":"exec_backends/#complete-hpc-workflow-example","title":"Complete HPC Workflow Example","text":"<p>Below is a complete example demonstrating how to execute workflows on HPC infrastructure using <code>RadicalExecutionBackend</code>.</p>"},{"location":"exec_backends/#setup-for-hpc-execution","title":"Setup for HPC Execution","text":"<pre><code>import time\nimport asyncio\nfrom radical.asyncflow import RadicalExecutionBackend\nfrom radical.asyncflow import WorkflowEngine\n\n# HPC backend configuration\nbackend = RadicalExecutionBackend({'resource': 'local.localhost'}) # (1)!\nflow = WorkflowEngine(backend=backend)\n</code></pre> <ol> <li>Configure for HPC execution - can target supercomputers, GPU clusters, local resources</li> </ol> <p>Tip</p> <p>HPC Resource Configuration: The <code>resource</code> parameter can be configured for various HPC systems: - <code>'local.localhost'</code> for local testing - <code>'ornl.summit'</code> for Oak Ridge Summit supercomputer - <code>'tacc.frontera'</code> for TACC Frontera system - <code>'anl.theta'</code> for Argonne Theta system - Custom configurations for your institutional clusters</p>"},{"location":"exec_backends/#define-executable-tasks-for-hpc","title":"Define Executable Tasks for HPC","text":"<pre><code>@flow.executable_task # (1)!\nasync def task1(*args):\n    return \"/bin/date\"\n\n@flow.executable_task\nasync def task2(*args):\n    return \"/bin/date\"\n\n@flow.executable_task\nasync def task3(*args):\n    return \"/bin/date\"\n</code></pre> <ol> <li><code>@flow.executable_task</code> creates tasks that execute as shell commands on HPC nodes</li> </ol> <p>Info</p> <p>Executable Tasks: Unlike function tasks, executable tasks return shell commands that are executed on remote HPC nodes, allowing you to leverage: - Specialized HPC software installed on compute nodes - High-performance compiled applications - GPU-accelerated programs - MPI-based parallel applications</p>"},{"location":"exec_backends/#define-workflow-with-dependencies","title":"Define Workflow with Dependencies","text":"<pre><code>async def run_wf(wf_id):\n    print(f'Starting workflow {wf_id} at {time.time()}')\n\n    # Create dependent task execution\n    t3 = task3(task1(), task2()) # (1)!\n    await t3  # Wait for distributed execution to complete\n\n    print(f'Workflow {wf_id} completed at {time.time()}')\n</code></pre> <ol> <li>Task3 depends on both task1 and task2 completion, but they execute on HPC nodes</li> </ol> <p>Note</p> <p>Dependency Handling: AsyncFlow automatically handles task dependencies across distributed HPC nodes, ensuring proper execution order while maximizing parallelism.</p>"},{"location":"exec_backends/#execute-multiple-workflows-on-hpc","title":"Execute Multiple Workflows on HPC","text":"<pre><code>start_time = time.time()\n\n# Execute 5 workflows concurrently across HPC infrastructure\nawait asyncio.gather(*[run_wf(i) for i in range(5)]) # (1)!\n\nend_time = time.time()\nprint(f'\\nTotal time running asynchronously is: {end_time - start_time}')\n\n# Proper cleanup of HPC resources\nawait flow.shutdown()\n</code></pre> <ol> <li>All workflows execute concurrently across available HPC nodes</li> </ol> HPC execution log <pre><code>RadicalExecutionBackend started\nStarting workflow 0 at 1752775108.50\nStarting workflow 1 at 1752775108.50\nStarting workflow 2 at 1752775108.50\nStarting workflow 3 at 1752775108.50\nStarting workflow 4 at 1752775108.50\nWorkflow 0 completed at 1752775110.25\nWorkflow 1 completed at 1752775110.26\nWorkflow 2 completed at 1752775110.27\nWorkflow 3 completed at 1752775110.28\nWorkflow 4 completed at 1752775110.29\nRadicalExecutionBackend: All tasks completed, cleaning up resources\n\nTotal time running asynchronously is: 1.79\n</code></pre>"},{"location":"exec_backends/#hpc-vs-local-development-key-differences","title":"HPC vs Local Development: Key Differences","text":"Aspect ConcurrentExecutionBackend RadicalExecutionBackend Scale Single machine, limited cores Thousands of nodes, massive parallelism Memory Local system RAM Distributed memory across nodes Storage Local filesystem High-performance parallel filesystems Task Type Python functions Shell executables, compiled programs Use Case Development, testing Production HPC workloads"},{"location":"exec_backends/#advanced-hpc-configurations","title":"Advanced HPC Configurations","text":""},{"location":"exec_backends/#gpu-cluster-configuration","title":"GPU Cluster Configuration","text":"<pre><code># Configure for GPU-accelerated computing\nbackend = RadicalExecutionBackend({\n    'resource': 'ornl.summit',\n    'queue': 'gpu',\n    'nodes': 100,\n    'gpus_per_node': 6,\n    'walltime': 120  # minutes\n})\n</code></pre>"},{"location":"exec_backends/#large-scale-cpu-configuration","title":"Large-Scale CPU Configuration","text":"<pre><code># Configure for massive CPU parallelism\nbackend = RadicalExecutionBackend({\n    'resource': 'tacc.frontera',\n    'queue': 'normal',\n    'nodes': 1000,\n    'cores_per_node': 56,\n    'walltime': 240  # minutes\n})\n</code></pre> <p>Warning</p> <p>Resource Management: Always call <code>await flow.shutdown()</code> to properly release HPC resources and prevent job queue issues.</p>"},{"location":"exec_backends/#real-world-hpc-use-cases","title":"Real-World HPC Use Cases","text":"<p>Scientific Computing: Execute thousands of simulations across supercomputer nodes for climate modeling, molecular dynamics, or astrophysics calculations.</p> <p>Machine Learning: Train large models across GPU clusters with distributed data processing and model parallelism.</p> <p>Bioinformatics: Process genomic datasets across hundreds of nodes for sequence alignment, variant calling, or phylogenetic analysis.</p> <p>Engineering Simulation: Run computational fluid dynamics, finite element analysis, or structural optimization across distributed computing resources.</p> <p>Tip</p> <p>Development Strategy: Start with <code>ConcurrentExecutionBackend</code> for local development and testing, then seamlessly switch to <code>RadicalExecutionBackend</code> for production HPC runs.</p>"},{"location":"exec_backends/#the-asyncflow-advantage","title":"The AsyncFlow Advantage","text":"<p>AsyncFlow's backend abstraction means your workflow logic remains identical whether running on: - Your laptop with 8 cores - A university cluster with 1,000 nodes - A national supercomputer with 100,000+ cores - GPU clusters with thousands of accelerators</p> <p>This write-once, run-anywhere approach dramatically reduces the complexity of scaling computational workflows from development to production HPC environments.</p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#asyncflow-installation-guide","title":"AsyncFlow \u2014 Installation Guide","text":"<p>AsyncFlow is a Python library built on top of <code>asyncio</code>, designed for orchestrating complex asynchronous workflows easily and reliably. This guide will help you install AsyncFlow in a clean Python environment.</p>"},{"location":"install/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python \u2265 3.8 (recommended: 3.11 or newer)</li> <li><code>pip</code> \u2265 22.0</li> <li>Optional: <code>conda</code> \u2265 4.10 for Conda environments</li> </ul> <p>Make sure Python is installed on your system:</p> <pre><code>python --version\npip --version\n</code></pre>"},{"location":"install/#recommended-create-a-clean-environment","title":"Recommended: Create a Clean Environment","text":"<p>It is strongly recommended to install AsyncFlow in an isolated environment (Conda or <code>venv</code>) to avoid conflicts with system packages.</p>"},{"location":"install/#install-with-conda","title":"Install with Conda","text":""},{"location":"install/#create-and-activate-a-conda-environment","title":"Create and activate a Conda environment:","text":"<pre><code>conda create -n asyncflow python=3.11 -y\nconda activate asyncflow\n</code></pre>"},{"location":"install/#install-asyncflow","title":"Install AsyncFlow:","text":"<pre><code>pip install asyncflow\n</code></pre>"},{"location":"install/#verify-installation","title":"Verify installation:","text":"<pre><code>python -c \"import radical.asyncflow; print('AsyncFlow installed \u2705')\"\n</code></pre>"},{"location":"install/#install-with-venv-built-in","title":"Install with venv (built-in)","text":"<p>If you don\u2019t use Conda, you can use Python\u2019s built-in <code>venv</code> module.</p>"},{"location":"install/#create-and-activate-a-virtual-environment","title":"Create and activate a virtual environment:","text":"<pre><code>python -m venv ~/.venvs/asyncflow\nsource ~/.venvs/asyncflow/bin/activate\n</code></pre> <p>(For Windows: <code>~/.venvs/asyncflow/Scripts/activate</code>)</p>"},{"location":"install/#install-asyncflow_1","title":"Install AsyncFlow:","text":"<pre><code>pip install asyncflow\n</code></pre>"},{"location":"install/#verify-installation_1","title":"Verify installation:","text":"<pre><code>python -c \"import radical.asyncflow; print('AsyncFlow installed \u2705')\"\n</code></pre>"},{"location":"install/#development-installation-optional","title":"Development Installation (optional)","text":"<p>If you want to contribute to AsyncFlow or use the latest code from GitHub:</p> <pre><code>git clone https://github.com/radical-cybertools/asyncflow.git@devel\ncd asyncflow\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs it in editable mode \u2014 any local changes you make to the code are reflected immediately.</p> <p>Tip</p> <ul> <li>Always activate your virtual environment before using AsyncFlow.</li> <li> <p>To deactivate an environment:</p> </li> <li> <p><code>conda deactivate</code> (Conda)</p> </li> <li><code>deactivate</code> (<code>venv</code>)</li> <li>You can list installed packages with <code>pip list</code>.</li> <li>It is a good practice to upgrade <code>pip</code> and <code>setuptools</code>:</li> </ul> <pre><code>pip install --upgrade pip setuptools\n</code></pre>"},{"location":"install/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li>Getting Started with AsyncFlow \u2192   Learn how to write your first workflow with AsyncFlow!</li> </ul>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>radical<ul> <li>asyncflow<ul> <li>backends<ul> <li>execution<ul> <li>base</li> <li>concurrent</li> <li>dask_parallel</li> <li>noop</li> <li>radical_pilot</li> </ul> </li> </ul> </li> <li>constants</li> <li>data</li> <li>errors</li> <li>logging</li> <li>utils</li> <li>workflow_manager</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/radical/asyncflow/constants/","title":"constants","text":""},{"location":"api/radical/asyncflow/constants/#radicalasyncflowconstants","title":"radical.asyncflow.constants","text":""},{"location":"api/radical/asyncflow/constants/#radical.asyncflow.constants.TasksMainStates","title":"TasksMainStates","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of standard task states used across all backends.</p> <p>This enum defines the canonical task states that are common to all execution backends, providing a unified interface for task state management.</p> <p>Attributes:</p> Name Type Description <code>DONE</code> <p>Task completed successfully.</p> <code>FAILED</code> <p>Task execution failed.</p> <code>CANCELED</code> <p>Task was canceled before completion.</p> <code>RUNNING</code> <p>Task is currently executing.</p>"},{"location":"api/radical/asyncflow/constants/#radical.asyncflow.constants.StateMapper","title":"StateMapper","text":"<pre><code>StateMapper(backend: Union[str, Any])\n</code></pre> <p>Unified interface for mapping task states between main workflow and    backend systems.</p> <p>StateMapper provides a centralized mechanism for translating task states between the main workflow system and various backend execution systems (e.g., 'radical', 'dask'). It supports dynamic registration of backend-specific state mappings and bidirectional conversion between main states and backend-specific states.</p> <p>The class maintains a registry of backend state mappings and provides methods for state conversion, backend detection, and direct state access through attribute notation.</p> <p>Attributes:</p> Name Type Description <code>_backend_registry</code> <code>dict[str, dict[TasksMainStates, Any]]</code> <p>Class-level registry mapping backend identifiers to their state mappings.</p> <code>backend_name</code> <code>str</code> <p>Name of the current backend.</p> <code>backend_module</code> <code>Optional[Any]</code> <p>Reference to the backend module/object.</p> <code>_state_map</code> <code>dict[TasksMainStates, Any]</code> <p>Current backend's state mappings.</p> <code>_reverse_map</code> <code>dict[Any, TasksMainStates]</code> <p>Reverse mapping from backend states to main states.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Union[str, Any]</code> <p>The backend identifier, either as a string (e.g., 'radical', 'dask') or as a backend module/object instance.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified backend is not registered or cannot be detected.</p> Example <p>::</p> <pre><code># Register a backend with custom states\nStateMapper.register_backend_states(\n    backend='my_backend',\n    done_state='COMPLETED',\n    failed_state='ERROR',\n    canceled_state='ABORTED',\n    running_state='ACTIVE'\n)\n\n# Use the mapper\nmapper = StateMapper('my_backend')\nbackend_state = mapper.DONE  # Returns 'COMPLETED'\n# Returns TasksMainStates.DONE\nmain_state = mapper.to_main_state('COMPLETED')\n</code></pre> <p>Initialize StateMapper with a specific backend.</p> <p>Creates a StateMapper instance configured for the specified backend. The backend can be provided as either a string identifier or a module/object instance.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Union[str, Any]</code> <p>Backend identifier. Can be: - String: Backend name like 'radical', 'dask', etc. - Object: Backend module or instance from which name is detected.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backend is not registered in the registry or if backend name cannot be detected from the provided object.</p> Note <p>The backend must be registered using register_backend_states() or register_backend_states_with_defaults() before initialization.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/constants.py</code> <pre><code>def __init__(self, backend: Union[str, Any]):\n    \"\"\"Initialize StateMapper with a specific backend.\n\n    Creates a StateMapper instance configured for the specified backend.\n    The backend can be provided as either a string identifier or a\n    module/object instance.\n\n    Args:\n        backend (Union[str, Any]): Backend identifier. Can be:\n            - String: Backend name like 'radical', 'dask', etc.\n            - Object: Backend module or instance from which name is detected.\n\n    Raises:\n        ValueError: If the backend is not registered in the registry or\n            if backend name cannot be detected from the provided object.\n\n    Note:\n        The backend must be registered using register_backend_states() or\n        register_backend_states_with_defaults() before initialization.\n    \"\"\"\n    self.backend_name: str\n    self.backend_module: Optional[Any] = None\n\n    if isinstance(backend, str):\n        self.backend_name = backend.lower()\n    else:\n        self.backend_module = backend\n        self.backend_name = self._detect_backend_name()\n\n    if self.backend_module not in self._backend_registry:\n        raise ValueError(\n            f\"Backend '{self.backend_module}' not registered. \"\n            f\"Available backends: {list(self._backend_registry.keys())}\"\n        )\n\n    self._state_map = self._backend_registry[self.backend_module]\n    self._reverse_map = {v: k for k, v in self._state_map.items()}\n</code></pre>"},{"location":"api/radical/asyncflow/constants/#radical.asyncflow.constants.StateMapper.terminal_states","title":"terminal_states  <code>property</code>","text":"<pre><code>terminal_states: tuple\n</code></pre> <p>Get all terminal states for the current backend.</p> <p>Returns a tuple containing the backend-specific representations of all terminal states (DONE, FAILED, CANCELED). These are states that indicate a task has finished execution and will not transition further.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>Backend-specific terminal state values (DONE, FAILED, CANCELED).</p> Example <p>::</p> <pre><code>mapper = StateMapper('my_backend')\nterminals = mapper.terminal_states\n# Returns ('COMPLETED', 'ERROR', 'ABORTED') for example backend\n</code></pre>"},{"location":"api/radical/asyncflow/constants/#radical.asyncflow.constants.StateMapper.register_backend_states","title":"register_backend_states  <code>classmethod</code>","text":"<pre><code>register_backend_states(backend: Any, done_state: Any, failed_state: Any, canceled_state: Any, running_state: Any, **additional_states) -&gt; None\n</code></pre> <p>Register state mappings for a new backend.</p> <p>Associates a backend identifier with its corresponding task state values, mapping the standard task states to backend-specific representations. Supports additional custom state mappings beyond the four core states.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Any</code> <p>The identifier for the backend to register. Can be a string, module, or any hashable object.</p> required <code>done_state</code> <code>Any</code> <p>Backend's representation of the DONE state.</p> required <code>failed_state</code> <code>Any</code> <p>Backend's representation of the FAILED state.</p> required <code>canceled_state</code> <code>Any</code> <p>Backend's representation of the CANCELED state.</p> required <code>running_state</code> <code>Any</code> <p>Backend's representation of the RUNNING state.</p> required <code>**additional_states</code> <p>Additional state mappings where the key is the state name (str) and the value is the backend's representation. Keys will be converted to uppercase and matched against TasksMainStates.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>::</p> <pre><code>StateMapper.register_backend_states(\n    backend='slurm',\n    done_state='COMPLETED',\n    failed_state='FAILED',\n    canceled_state='CANCELLED',\n    running_state='RUNNING',\n    pending='PENDING',\n    timeout='TIMEOUT'\n)\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/constants.py</code> <pre><code>@classmethod\ndef register_backend_states(\n    cls,\n    backend: Any,\n    done_state: Any,\n    failed_state: Any,\n    canceled_state: Any,\n    running_state: Any,\n    **additional_states,\n) -&gt; None:\n    \"\"\"Register state mappings for a new backend.\n\n    Associates a backend identifier with its corresponding task state values,\n    mapping the standard task states to backend-specific representations.\n    Supports additional custom state mappings beyond the four core states.\n\n    Args:\n        backend (Any): The identifier for the backend to register. Can be\n            a string, module, or any hashable object.\n        done_state (Any): Backend's representation of the DONE state.\n        failed_state (Any): Backend's representation of the FAILED state.\n        canceled_state (Any): Backend's representation of the CANCELED state.\n        running_state (Any): Backend's representation of the RUNNING state.\n        **additional_states: Additional state mappings where the key is the\n            state name (str) and the value is the backend's representation.\n            Keys will be converted to uppercase and matched against TasksMainStates.\n\n    Returns:\n        None\n\n    Example:\n        ::\n\n            StateMapper.register_backend_states(\n                backend='slurm',\n                done_state='COMPLETED',\n                failed_state='FAILED',\n                canceled_state='CANCELLED',\n                running_state='RUNNING',\n                pending='PENDING',\n                timeout='TIMEOUT'\n            )\n    \"\"\"\n    additional_mapped = {\n        TasksMainStates[k.upper()]: v for k, v in additional_states.items()\n    }\n    cls._backend_registry[backend] = {\n        TasksMainStates.DONE: done_state,\n        TasksMainStates.FAILED: failed_state,\n        TasksMainStates.CANCELED: canceled_state,\n        TasksMainStates.RUNNING: running_state,\n        **additional_mapped,\n    }\n</code></pre>"},{"location":"api/radical/asyncflow/constants/#radical.asyncflow.constants.StateMapper.register_backend_states_with_defaults","title":"register_backend_states_with_defaults  <code>classmethod</code>","text":"<pre><code>register_backend_states_with_defaults(backend: Any)\n</code></pre> <p>Register a backend using default main state values.</p> <p>Convenience method that registers a backend where the backend-specific states are identical to the main state values (i.e., the string values of the TasksMainStates enum).</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Any</code> <p>The backend identifier to register.</p> required <p>Returns:</p> Type Description <p>The result of register_backend_states() with default values.</p> Example <p>::</p> <pre><code># This registers backend states as:\n# DONE -&gt; \"DONE\", FAILED -&gt; \"FAILED\", etc.\nStateMapper.register_backend_states_with_defaults('thread_backend')\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/constants.py</code> <pre><code>@classmethod\ndef register_backend_states_with_defaults(cls, backend: Any):\n    \"\"\"Register a backend using default main state values.\n\n    Convenience method that registers a backend where the backend-specific\n    states are identical to the main state values (i.e., the string values\n    of the TasksMainStates enum).\n\n    Args:\n        backend (Any): The backend identifier to register.\n\n    Returns:\n        The result of register_backend_states() with default values.\n\n    Example:\n        ::\n\n            # This registers backend states as:\n            # DONE -&gt; \"DONE\", FAILED -&gt; \"FAILED\", etc.\n            StateMapper.register_backend_states_with_defaults('thread_backend')\n    \"\"\"\n    return cls.register_backend_states(\n        backend,\n        done_state=TasksMainStates.DONE.value,\n        failed_state=TasksMainStates.FAILED.value,\n        canceled_state=TasksMainStates.CANCELED.value,\n        running_state=TasksMainStates.RUNNING.value,\n    )\n</code></pre>"},{"location":"api/radical/asyncflow/constants/#radical.asyncflow.constants.StateMapper.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name: str) -&gt; Any\n</code></pre> <p>Access backend-specific states directly via attribute notation.</p> <p>Enables direct access to backend states using the main state names as attributes (e.g., mapper.DONE, mapper.FAILED).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The main state name to access (DONE, FAILED, CANCELED, RUNNING).</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The backend-specific state value corresponding to the main state.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the specified state name is not valid.</p> Example <p>::</p> <pre><code>mapper = StateMapper('my_backend')\ndone_state = mapper.DONE  # Returns backend's DONE state\nrunning_state = mapper.RUNNING  # Returns backend's RUNNING state\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/constants.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Access backend-specific states directly via attribute notation.\n\n    Enables direct access to backend states using the main state names\n    as attributes (e.g., mapper.DONE, mapper.FAILED).\n\n    Args:\n        name (str): The main state name to access (DONE, FAILED, CANCELED, RUNNING).\n\n    Returns:\n        Any: The backend-specific state value corresponding to the main state.\n\n    Raises:\n        AttributeError: If the specified state name is not valid.\n\n    Example:\n        ::\n\n            mapper = StateMapper('my_backend')\n            done_state = mapper.DONE  # Returns backend's DONE state\n            running_state = mapper.RUNNING  # Returns backend's RUNNING state\n    \"\"\"\n    try:\n        main_state = TasksMainStates[name]\n        return self._state_map[main_state]\n    except KeyError:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' has no state '{name}'\"\n        ) from None\n</code></pre>"},{"location":"api/radical/asyncflow/constants/#radical.asyncflow.constants.StateMapper.to_main_state","title":"to_main_state","text":"<pre><code>to_main_state(backend_state: Any) -&gt; TasksMainStates\n</code></pre> <p>Convert backend-specific state to main state.</p> <p>Translates a backend-specific state value back to the corresponding TasksMainStates enum value.</p> <p>Parameters:</p> Name Type Description Default <code>backend_state</code> <code>Any</code> <p>The backend-specific state value to convert.</p> required <p>Returns:</p> Name Type Description <code>TasksMainStates</code> <code>TasksMainStates</code> <p>The corresponding main state enum value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backend state is not recognized.</p> Example <p>::</p> <pre><code>mapper = StateMapper('slurm')\nmain_state = mapper.to_main_state('COMPLETED')  # TasksMainStates.DONE\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/constants.py</code> <pre><code>def to_main_state(self, backend_state: Any) -&gt; TasksMainStates:\n    \"\"\"Convert backend-specific state to main state.\n\n    Translates a backend-specific state value back to the corresponding\n    TasksMainStates enum value.\n\n    Args:\n        backend_state (Any): The backend-specific state value to convert.\n\n    Returns:\n        TasksMainStates: The corresponding main state enum value.\n\n    Raises:\n        ValueError: If the backend state is not recognized.\n\n    Example:\n        ::\n\n            mapper = StateMapper('slurm')\n            main_state = mapper.to_main_state('COMPLETED')  # TasksMainStates.DONE\n    \"\"\"\n    try:\n        return self._reverse_map[backend_state]\n    except KeyError:\n        raise ValueError(f\"Unknown backend state: {backend_state}\") from None\n</code></pre>"},{"location":"api/radical/asyncflow/constants/#radical.asyncflow.constants.StateMapper.get_backend_state","title":"get_backend_state","text":"<pre><code>get_backend_state(main_state: Union[TasksMainStates, str]) -&gt; Any\n</code></pre> <p>Get backend-specific state for a main state.</p> <p>Retrieves the backend-specific state value that corresponds to the given main state. Accepts both TasksMainStates enum values and string representations.</p> <p>Parameters:</p> Name Type Description Default <code>main_state</code> <code>Union[TasksMainStates, str]</code> <p>The main state to convert. Can be a TasksMainStates enum value or its string representation.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The backend-specific state value.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the main state is not found in the mapping.</p> Example <p>::</p> <pre><code>mapper = StateMapper('my_backend')\nbackend_state = mapper.get_backend_state(TasksMainStates.DONE)\n# Or using string\nbackend_state = mapper.get_backend_state('DONE')\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/constants.py</code> <pre><code>def get_backend_state(self, main_state: Union[TasksMainStates, str]) -&gt; Any:\n    \"\"\"Get backend-specific state for a main state.\n\n    Retrieves the backend-specific state value that corresponds to the\n    given main state. Accepts both TasksMainStates enum values and\n    string representations.\n\n    Args:\n        main_state (Union[TasksMainStates, str]): The main state to convert.\n            Can be a TasksMainStates enum value or its string representation.\n\n    Returns:\n        Any: The backend-specific state value.\n\n    Raises:\n        KeyError: If the main state is not found in the mapping.\n\n    Example:\n        ::\n\n            mapper = StateMapper('my_backend')\n            backend_state = mapper.get_backend_state(TasksMainStates.DONE)\n            # Or using string\n            backend_state = mapper.get_backend_state('DONE')\n    \"\"\"\n    if isinstance(main_state, str):\n        main_state = TasksMainStates[main_state]\n    return self._state_map[main_state]\n</code></pre>"},{"location":"api/radical/asyncflow/data/","title":"data","text":""},{"location":"api/radical/asyncflow/data/#radicalasyncflowdata","title":"radical.asyncflow.data","text":""},{"location":"api/radical/asyncflow/data/#radical.asyncflow.data.File","title":"File","text":"<pre><code>File()\n</code></pre> <p>Base class for file handling in task execution systems.</p> <p>Provides common attributes and functionality for managing files with filename and filepath properties.</p> <p>Initialize a File object with default None values.</p> <p>Sets filename and filepath attributes to None, to be populated by subclasses during file resolution.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/data.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize a File object with default None values.\n\n    Sets filename and filepath attributes to None, to be populated\n    by subclasses during file resolution.\n    \"\"\"\n    self.filename = None\n    self.filepath = None\n</code></pre>"},{"location":"api/radical/asyncflow/data/#radical.asyncflow.data.File.download_remote_url","title":"download_remote_url  <code>staticmethod</code>","text":"<pre><code>download_remote_url(url: str) -&gt; Path\n</code></pre> <p>Download a remote file to the current directory and return its full path.</p> <p>Downloads file content from a remote URL using streaming to handle large files efficiently. Saves the file with a name derived from the URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The remote URL to download from.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Absolute path to the downloaded file.</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>If the download fails</p> Example <p>::</p> <pre><code>file_path = File.download_remote_url(\"https://example.com/data.txt\")\nprint(f\"Downloaded to: {file_path}\")\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/data.py</code> <pre><code>@staticmethod\ndef download_remote_url(url: str) -&gt; Path:\n    \"\"\"Download a remote file to the current directory and return its full path.\n\n    Downloads file content from a remote URL using streaming to handle large files\n    efficiently. Saves the file with a name derived from the URL.\n\n    Args:\n        url: The remote URL to download from.\n\n    Returns:\n        Path: Absolute path to the downloaded file.\n\n    Raises:\n        requests.exceptions.RequestException: If the download fails\n        or URL is invalid.\n\n    Example:\n        ::\n\n            file_path = File.download_remote_url(\"https://example.com/data.txt\")\n            print(f\"Downloaded to: {file_path}\")\n    \"\"\"\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Check if the download was successful\n\n    # Use the file name from the URL, defaulting if not available\n    filename = url.split(\"/\")[-1] or \"downloaded_file\"\n    file_path = Path(filename)\n\n    # Save the file content\n    with open(file_path, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n\n    return file_path.resolve()  # Return the absolute path\n</code></pre>"},{"location":"api/radical/asyncflow/data/#radical.asyncflow.data.InputFile","title":"InputFile","text":"<pre><code>InputFile(file)\n</code></pre> <p>               Bases: <code>File</code></p> <p>Represents an input file that can be sourced from remote URLs, local paths,    or task outputs.</p> <p>Automatically detects the file source type and handles appropriate resolution. Supports remote file downloading, local file path resolution, and task output file references.</p> <p>Initialize an InputFile with automatic source type detection and resolution.</p> <p>Determines whether the input is a remote URL, local file path, or reference to another task's output file, then resolves the appropriate file path.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <p>Input file specification. Can be: - Remote URL (http, https, ftp, s3, etc.) - Local file path (absolute or relative) - Task output file reference (filename for future resolution)</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If file resolution fails or file source cannot be determined.</p> <p>Attributes:</p> Name Type Description <code>remote_url</code> <code>str</code> <p>URL if file is remote, None otherwise.</p> <code>local_file</code> <code>str</code> <p>Local path if file exists locally, None otherwise.</p> <code>other_task_file</code> <code>str</code> <p>Task reference if file is from another task.</p> <code>filepath</code> <code>Path</code> <p>Resolved file path.</p> <code>filename</code> <code>str</code> <p>Extracted filename from the resolved path.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/data.py</code> <pre><code>def __init__(self, file):\n    \"\"\"Initialize an InputFile with automatic source type detection and resolution.\n\n    Determines whether the input is a remote URL, local file path, or reference\n    to another task's output file, then resolves the appropriate file path.\n\n    Args:\n        file: Input file specification. Can be:\n            - Remote URL (http, https, ftp, s3, etc.)\n            - Local file path (absolute or relative)\n            - Task output file reference (filename for future resolution)\n\n    Raises:\n        Exception: If file resolution fails or file source cannot be determined.\n\n    Attributes:\n        remote_url (str): URL if file is remote, None otherwise.\n        local_file (str): Local path if file exists locally, None otherwise.\n        other_task_file (str): Task reference if file is from another task.\n        filepath (Path): Resolved file path.\n        filename (str): Extracted filename from the resolved path.\n    \"\"\"\n    # Initialize file-related variables\n    self.remote_url = None\n    self.local_file = None\n    self.other_task_file = None\n\n    self.filepath = None  # Ensure that filepath is initialized\n\n    # Determine file type (remote, local, or task-produced)\n    parsed_url = urlparse(file)\n    if parsed_url.scheme in URL_SCHEMES:\n        self.remote_url = file\n    elif os.path.exists(file):  # Check if it's a local file\n        self.local_file = file\n    else:\n        self.other_task_file = file\n\n    # Handle remote file (download and resolve path)\n    if self.remote_url:\n        self.filepath = self.download_remote_url(self.remote_url)\n\n    # Handle local file (ensure it exists and resolve path)\n    elif self.local_file:\n        self.filepath = Path(self.local_file).resolve()  # Convert to absolute path\n\n    # Handle file from another task. We do not resolve Path here as this\n    # file is not created yet and it will be resolved when the task is executed.\n    elif self.other_task_file:\n        self.filepath = Path(self.other_task_file)\n\n    # If file resolution failed, raise an exception with a more descriptive message\n    if not self.filepath:\n        raise Exception(\n            f\"Failed to resolve InputFile: {file}. \"\n            \"Ensure it's a valid URL, local path, or task output.\"\n        )\n\n    # Set the filename from the resolved filepath\n    self.filename = self.filepath.name\n</code></pre>"},{"location":"api/radical/asyncflow/data/#radical.asyncflow.data.InputFile.download_remote_url","title":"download_remote_url  <code>staticmethod</code>","text":"<pre><code>download_remote_url(url: str) -&gt; Path\n</code></pre> <p>Download a remote file to the current directory and return its full path.</p> <p>Downloads file content from a remote URL using streaming to handle large files efficiently. Saves the file with a name derived from the URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The remote URL to download from.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Absolute path to the downloaded file.</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>If the download fails</p> Example <p>::</p> <pre><code>file_path = File.download_remote_url(\"https://example.com/data.txt\")\nprint(f\"Downloaded to: {file_path}\")\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/data.py</code> <pre><code>@staticmethod\ndef download_remote_url(url: str) -&gt; Path:\n    \"\"\"Download a remote file to the current directory and return its full path.\n\n    Downloads file content from a remote URL using streaming to handle large files\n    efficiently. Saves the file with a name derived from the URL.\n\n    Args:\n        url: The remote URL to download from.\n\n    Returns:\n        Path: Absolute path to the downloaded file.\n\n    Raises:\n        requests.exceptions.RequestException: If the download fails\n        or URL is invalid.\n\n    Example:\n        ::\n\n            file_path = File.download_remote_url(\"https://example.com/data.txt\")\n            print(f\"Downloaded to: {file_path}\")\n    \"\"\"\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Check if the download was successful\n\n    # Use the file name from the URL, defaulting if not available\n    filename = url.split(\"/\")[-1] or \"downloaded_file\"\n    file_path = Path(filename)\n\n    # Save the file content\n    with open(file_path, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n\n    return file_path.resolve()  # Return the absolute path\n</code></pre>"},{"location":"api/radical/asyncflow/data/#radical.asyncflow.data.OutputFile","title":"OutputFile","text":"<pre><code>OutputFile(filename)\n</code></pre> <p>               Bases: <code>File</code></p> <p>Represents an output file that will be produced by a task.</p> <p>Handles filename validation and extraction from file paths, ensuring proper output file naming for task execution.</p> <p>Initialize an OutputFile with filename validation.</p> <p>Extracts the filename from the provided path and validates that it represents a valid file (not a directory or empty path).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>The output filename or path. Can be a simple filename or a path, but must resolve to a valid filename.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If filename is empty or resolves to an invalid file path.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The extracted filename for the output file.</p> Example <p>::</p> <pre><code># Valid initializations\noutput1 = OutputFile(\"result.txt\")\noutput2 = OutputFile(\"path/to/result.txt\")\n\n# Invalid - will raise ValueError\noutput3 = OutputFile(\"\")  # Empty filename\noutput4 = OutputFile(\"path/\")  # Path ends with separator\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/data.py</code> <pre><code>def __init__(self, filename):\n    \"\"\"Initialize an OutputFile with filename validation.\n\n    Extracts the filename from the provided path and validates that it\n    represents a valid file (not a directory or empty path).\n\n    Args:\n        filename: The output filename or path. Can be a simple filename\n            or a path, but must resolve to a valid filename.\n\n    Raises:\n        ValueError: If filename is empty or resolves to an invalid file path.\n\n    Attributes:\n        filename (str): The extracted filename for the output file.\n\n    Example:\n        ::\n\n            # Valid initializations\n            output1 = OutputFile(\"result.txt\")\n            output2 = OutputFile(\"path/to/result.txt\")\n\n            # Invalid - will raise ValueError\n            output3 = OutputFile(\"\")  # Empty filename\n            output4 = OutputFile(\"path/\")  # Path ends with separator\n    \"\"\"\n    if not filename:\n        raise ValueError(\"Filename cannot be empty\")\n\n    # Use os.path.basename() to handle paths\n    self.filename = os.path.basename(filename)\n\n    # Edge case: If the filename ends with a separator (e.g., '/')\n    if not self.filename:\n        raise ValueError(\n            f\"Invalid filename, the path {filename} does not include a file\"\n        )\n</code></pre>"},{"location":"api/radical/asyncflow/data/#radical.asyncflow.data.OutputFile.download_remote_url","title":"download_remote_url  <code>staticmethod</code>","text":"<pre><code>download_remote_url(url: str) -&gt; Path\n</code></pre> <p>Download a remote file to the current directory and return its full path.</p> <p>Downloads file content from a remote URL using streaming to handle large files efficiently. Saves the file with a name derived from the URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The remote URL to download from.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Absolute path to the downloaded file.</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>If the download fails</p> Example <p>::</p> <pre><code>file_path = File.download_remote_url(\"https://example.com/data.txt\")\nprint(f\"Downloaded to: {file_path}\")\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/data.py</code> <pre><code>@staticmethod\ndef download_remote_url(url: str) -&gt; Path:\n    \"\"\"Download a remote file to the current directory and return its full path.\n\n    Downloads file content from a remote URL using streaming to handle large files\n    efficiently. Saves the file with a name derived from the URL.\n\n    Args:\n        url: The remote URL to download from.\n\n    Returns:\n        Path: Absolute path to the downloaded file.\n\n    Raises:\n        requests.exceptions.RequestException: If the download fails\n        or URL is invalid.\n\n    Example:\n        ::\n\n            file_path = File.download_remote_url(\"https://example.com/data.txt\")\n            print(f\"Downloaded to: {file_path}\")\n    \"\"\"\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Check if the download was successful\n\n    # Use the file name from the URL, defaulting if not available\n    filename = url.split(\"/\")[-1] or \"downloaded_file\"\n    file_path = Path(filename)\n\n    # Save the file content\n    with open(file_path, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n\n    return file_path.resolve()  # Return the absolute path\n</code></pre>"},{"location":"api/radical/asyncflow/errors/","title":"errors","text":""},{"location":"api/radical/asyncflow/errors/#radicalasyncflowerrors","title":"radical.asyncflow.errors","text":""},{"location":"api/radical/asyncflow/errors/#radical.asyncflow.errors.DependencyFailureError","title":"DependencyFailureError","text":"<pre><code>DependencyFailureError(message, failed_dependencies=None, root_cause=None)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when a workflow component cannot execute due to dependency    failures.</p> <p>This exception provides detailed information about the failed dependencies and maintains the chain of causation for debugging purposes.</p> <p>Initialize DependencyFailureError exception.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Human-readable error message</p> required <code>failed_dependencies</code> <code>list</code> <p>List of failed dependency names</p> <code>None</code> <code>root_cause</code> <code>Exception</code> <p>The original exception that caused</p> <code>None</code> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/errors.py</code> <pre><code>def __init__(self, message, failed_dependencies=None, root_cause=None):\n    \"\"\"Initialize DependencyFailureError exception.\n\n    Args:\n        message (str): Human-readable error message\n        failed_dependencies (list, optional): List of failed dependency names\n        root_cause (Exception, optional): The original exception that caused\n        the failure\n    \"\"\"\n    super().__init__(message)\n    self.failed_dependencies = failed_dependencies or []\n    self.root_cause = root_cause\n\n    # Chain the exception if we have a root cause\n    if root_cause:\n        self.__cause__ = root_cause\n</code></pre>"},{"location":"api/radical/asyncflow/logging/","title":"logging","text":""},{"location":"api/radical/asyncflow/logging/#radicalasyncflowlogging","title":"radical.asyncflow.logging","text":""},{"location":"api/radical/asyncflow/logging/#radical.asyncflow.logging.StructuredLogger","title":"StructuredLogger","text":"<pre><code>StructuredLogger(logger: Logger)\n</code></pre> <p>Drop-in replacement that adds structured logging capabilities.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/logging.py</code> <pre><code>def __init__(self, logger: logging.Logger):\n    self._logger = logger\n</code></pre>"},{"location":"api/radical/asyncflow/logging/#radical.asyncflow.logging.StructuredLogger.info","title":"info","text":"<pre><code>info(message: str, **kwargs)\n</code></pre> <p>Enhanced info() that accepts both old and new usage patterns.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/logging.py</code> <pre><code>def info(self, message: str, **kwargs):\n    \"\"\"Enhanced info() that accepts both old and new usage patterns.\"\"\"\n    if kwargs:\n        self._logger.info(message, extra=kwargs)\n    else:\n        self._logger.info(message)\n</code></pre>"},{"location":"api/radical/asyncflow/logging/#radical.asyncflow.logging.init_default_logger","title":"init_default_logger","text":"<pre><code>init_default_logger(log_level: Union[int, str] = INFO, *, output_file: Optional[Union[str, Path]] = None, file_log_level: Optional[Union[int, str]] = None, use_colors: bool = True, show_details: bool = False, style: str = 'modern', clear_handlers: bool = False, logger_name: Optional[str] = None, structured_logging: bool = False, structured_file: Optional[Union[str, Path]] = None) -&gt; Logger\n</code></pre> <p>Setup and configure logging with enhanced features.</p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <code>Union[int, str]</code> <p>Base logging level for console output.</p> <code>INFO</code> <code>output_file</code> <code>Optional[Union[str, Path]]</code> <p>Path to log file. If provided, file logging is enabled.</p> <code>None</code> <code>file_log_level</code> <code>Optional[Union[int, str]]</code> <p>Logging level for file output. Defaults to log_level.</p> <code>None</code> <code>use_colors</code> <code>bool</code> <p>Enable colored console output.</p> <code>True</code> <code>show_details</code> <code>bool</code> <p>Include thread/process info in log messages.</p> <code>False</code> <code>style</code> <code>str</code> <p>Format style - 'modern', 'core', 'execution_backend', or 'minimal'.</p> <code>'modern'</code> <code>clear_handlers</code> <code>bool</code> <p>Remove existing handlers from root logger.</p> <code>False</code> <code>logger_name</code> <code>Optional[str]</code> <p>Name for the logger. If None, returns root logger.</p> <code>None</code> <code>structured_logging</code> <code>bool</code> <p>Enable JSON structured logging to file.</p> <code>False</code> <code>structured_file</code> <code>Optional[Union[str, Path]]</code> <p>Custom path for structured JSON log file.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/logging.py</code> <pre><code>def init_default_logger(\n    log_level: Union[int, str] = logging.INFO,\n    *,\n    output_file: Optional[Union[str, pathlib.Path]] = None,\n    file_log_level: Optional[Union[int, str]] = None,\n    use_colors: bool = True,\n    show_details: bool = False,\n    style: str = \"modern\",\n    clear_handlers: bool = False,\n    logger_name: Optional[str] = None,\n    structured_logging: bool = False,\n    structured_file: Optional[Union[str, pathlib.Path]] = None,\n) -&gt; logging.Logger:\n    \"\"\"Setup and configure logging with enhanced features.\n\n    Args:\n        log_level: Base logging level for console output.\n        output_file: Path to log file. If provided, file logging is enabled.\n        file_log_level: Logging level for file output. Defaults to log_level.\n        use_colors: Enable colored console output.\n        show_details: Include thread/process info in log messages.\n        style: Format style - 'modern', 'core', 'execution_backend', or 'minimal'.\n        clear_handlers: Remove existing handlers from root logger.\n        logger_name: Name for the logger. If None, returns root logger.\n        structured_logging: Enable JSON structured logging to file.\n        structured_file: Custom path for structured JSON log file.\n\n    Returns:\n        Configured logger instance.\n    \"\"\"\n    # Get or create logger\n    logger = logging.getLogger(logger_name) if logger_name else logging.getLogger()\n\n    # Clear existing handlers if requested\n    if clear_handlers:\n        logger.handlers.clear()\n\n    # Console always uses colored formatter (never JSON)\n    console_formatter = _ColoredFormatter(\n        use_colors=use_colors, show_details=show_details, style=style\n    )\n\n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setFormatter(console_formatter)\n    console_handler.setLevel(log_level)\n\n    if show_details:\n        console_handler.addFilter(_thread_info_filter)\n\n    logger.addHandler(console_handler)\n\n    # Regular file handler (if requested) - uses colored formatter without colors\n    if output_file is not None:\n        file_level = log_level if file_log_level is None else file_log_level\n        file_path = pathlib.Path(output_file)\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n\n        file_handler = logging.FileHandler(file_path)\n        file_formatter = _ColoredFormatter(\n            use_colors=False,  # No colors in file\n            show_details=show_details,\n            style=style,\n        )\n        file_handler.setFormatter(file_formatter)\n        file_handler.setLevel(file_level)\n\n        if show_details:\n            file_handler.addFilter(_thread_info_filter)\n\n        logger.addHandler(file_handler)\n\n    # Structured JSON file handler (if requested)\n    if structured_logging:\n        # Auto-generate structured log file name if not provided\n        if structured_file is None:\n            if output_file is not None:\n                # Use same directory as regular log file\n                structured_file = pathlib.Path(output_file).with_suffix(\".json\")\n            else:\n                # Default to structured.log in current directory\n                structured_file = \"radical.asyncflow.logs.json\"\n\n        struct_level = log_level if file_log_level is None else file_log_level\n        struct_path = pathlib.Path(structured_file)\n        struct_path.parent.mkdir(parents=True, exist_ok=True)\n\n        struct_handler = logging.FileHandler(struct_path)\n        struct_formatter = _StructuredFormatter()\n        struct_handler.setFormatter(struct_formatter)\n        struct_handler.setLevel(struct_level)\n\n        if show_details:\n            struct_handler.addFilter(_thread_info_filter)\n\n        logger.addHandler(struct_handler)\n\n    # Set logger level\n    logger.setLevel(logging.NOTSET)\n\n    # Capture warnings\n    logging.captureWarnings(True)\n\n    # Log configuration info\n    level_name = (\n        logging.getLevelName(log_level) if isinstance(log_level, int) else log_level\n    )\n    file_level_name = (\n        logging.getLevelName(file_log_level)\n        if isinstance(file_log_level, int)\n        else file_log_level\n    )\n\n    structured_info = str(structured_file) if structured_logging else \"disabled\"\n\n    logger.info(\n        \"Logger configured successfully - \"\n        \"Console: %s, File: %s (%s), Structured: %s, Style: %s\",\n        level_name,\n        output_file or \"disabled\",\n        file_level_name or \"N/A\",\n        structured_info,\n        style,\n    )\n\n    return logger\n</code></pre>"},{"location":"api/radical/asyncflow/logging/#radical.asyncflow.logging.get_structured_logger","title":"get_structured_logger","text":"<pre><code>get_structured_logger(name: str = None, level: Union[int, str] = INFO) -&gt; StructuredLogger\n</code></pre> <p>Get a structured logger that supports both old and new usage patterns.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/logging.py</code> <pre><code>def get_structured_logger(\n    name: str = None, level: Union[int, str] = logging.INFO\n) -&gt; StructuredLogger:\n    \"\"\"Get a structured logger that supports both old and new usage patterns.\"\"\"\n    # Check if logger already configured, if not configure it with structured logging\n    base_logger = logging.getLogger(name)\n    if not base_logger.handlers:\n        init_default_logger(level, logger_name=name, structured_logging=True)\n        base_logger = logging.getLogger(name)\n\n    return StructuredLogger(base_logger)\n</code></pre>"},{"location":"api/radical/asyncflow/logging/#radical.asyncflow.logging.get_logger","title":"get_logger","text":"<pre><code>get_logger(name: str = None, level: Union[int, str] = INFO) -&gt; Logger\n</code></pre> <p>Quick logger setup for simple use cases.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/logging.py</code> <pre><code>def get_logger(\n    name: str = None, level: Union[int, str] = logging.INFO\n) -&gt; logging.Logger:\n    \"\"\"Quick logger setup for simple use cases.\"\"\"\n    if not logging.getLogger().handlers:\n        init_default_logger(level, logger_name=name, style=\"modern\")\n    return logging.getLogger(name)\n</code></pre>"},{"location":"api/radical/asyncflow/utils/","title":"utils","text":""},{"location":"api/radical/asyncflow/utils/#radicalasyncflowutils","title":"radical.asyncflow.utils","text":""},{"location":"api/radical/asyncflow/utils/#radical.asyncflow.utils.get_next_uid","title":"get_next_uid","text":"<pre><code>get_next_uid()\n</code></pre> <p>Return the next unique task ID.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/utils.py</code> <pre><code>def get_next_uid():\n    \"\"\"Return the next unique task ID.\"\"\"\n    global _global_task_counter\n    _global_task_counter += 1\n    return f\"{_global_task_counter:06d}\"\n</code></pre>"},{"location":"api/radical/asyncflow/utils/#radical.asyncflow.utils.reset_uid_counter","title":"reset_uid_counter","text":"<pre><code>reset_uid_counter()\n</code></pre> <p>Reset the counter to zero (only call when backend shuts down).</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/utils.py</code> <pre><code>def reset_uid_counter():\n    \"\"\"Reset the counter to zero (only call when backend shuts down).\"\"\"\n    global _global_task_counter\n    _global_task_counter = 0\n</code></pre>"},{"location":"api/radical/asyncflow/utils/#radical.asyncflow.utils.get_event_loop_or_raise","title":"get_event_loop_or_raise","text":"<pre><code>get_event_loop_or_raise(context_name: str = 'AsyncWorkflowEngine') -&gt; AbstractEventLoop\n</code></pre> <p>Get the current running event loop or raise a helpful error.</p> <p>Parameters:</p> Name Type Description Default <code>context_name</code> <code>str</code> <p>Name of the class/context for error messages</p> <code>'AsyncWorkflowEngine'</code> <p>Returns:</p> Type Description <code>AbstractEventLoop</code> <p>asyncio.AbstractEventLoop: The current running event loop</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no event loop is running with helpful guidance</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/utils.py</code> <pre><code>def get_event_loop_or_raise(\n    context_name: str = \"AsyncWorkflowEngine\",\n) -&gt; asyncio.AbstractEventLoop:\n    \"\"\"\n    Get the current running event loop or raise a helpful error.\n\n    Args:\n        context_name: Name of the class/context for error messages\n\n    Returns:\n        asyncio.AbstractEventLoop: The current running event loop\n\n    Raises:\n        RuntimeError: If no event loop is running with helpful guidance\n    \"\"\"\n    try:\n        return asyncio.get_running_loop()\n    except RuntimeError as e:\n        raise RuntimeError(\n            f\"{context_name} must be created within an async context. \"\n            f\"Use 'await {context_name}.create(...)' from within an async function, \"\n            \"or run within asyncio.run().\"\n        ) from e\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/","title":"workflow_manager","text":""},{"location":"api/radical/asyncflow/workflow_manager/#radicalasyncflowworkflow_manager","title":"radical.asyncflow.workflow_manager","text":""},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine","title":"WorkflowEngine","text":"<pre><code>WorkflowEngine(backend: BaseExecutionBackend, dry_run: bool = False, implicit_data: bool = True)\n</code></pre> <p>An asynchronous workflow manager that uses asyncio event loops and coroutines to manage and execute workflow components (blocks and/or tasks) within Directed Acyclic Graph (DAG) or Chain Graph (CG) structures.</p> <p>This class provides async/await operations and handles task dependencies, input/output data staging, and execution.</p> <p>Attributes:</p> Name Type Description <code>loop</code> <code>AbstractEventLoop</code> <p>The asyncio event loop (current running loop).</p> <code>backend</code> <code>BaseExecutionBackend</code> <p>The execution backend used for task execution.</p> <code>dry_run</code> <code>bool</code> <p>Indicates whether the engine is in dry-run mode.</p> <code>work_dir</code> <code>str</code> <p>The working directory for the workflow session.</p> <code>log</code> <code>Logger</code> <p>Logger instance for logging workflow events.</p> <code>prof</code> <code>Profiler</code> <p>Profiler instance for profiling workflow execution.</p> <p>Initialize the WorkflowEngine (sync part only).</p> <p>Note: This is a private constructor. Use WorkflowEngine.create() instead.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>BaseExecutionBackend</code> <p>Execution backend (required, pre-validated)</p> required <code>dry_run</code> <code>bool</code> <p>Whether to run in dry-run mode</p> <code>False</code> <code>implicit_data</code> <code>bool</code> <p>Whether to enable implicit data dependency linking</p> <code>True</code> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>@typeguard.typechecked\ndef __init__(\n    self,\n    backend: BaseExecutionBackend,\n    dry_run: bool = False,\n    implicit_data: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize the WorkflowEngine (sync part only).\n\n    Note: This is a private constructor. Use WorkflowEngine.create() instead.\n\n    Args:\n        backend: Execution backend (required, pre-validated)\n        dry_run: Whether to run in dry-run mode\n        implicit_data: Whether to enable implicit data dependency linking\n    \"\"\"\n    # Get the current running loop - assume it exists\n    self.loop = get_event_loop_or_raise(\"WorkflowEngine\")\n\n    # Store backend (already validated by create method)\n    self.backend = backend\n\n    # Initialize core attributes\n    self.running = []\n    self.components = {}\n    self.resolved = set()\n    self.dependencies = {}\n    self.dry_run = dry_run\n    self.queue = asyncio.Queue()\n    self.implicit_data_mode = implicit_data\n\n    # Optimization: Track component state changes\n    self._ready_queue = deque()\n    self._dependents_map = defaultdict(set)\n    self._dependency_count = {}\n    self._component_change_event = asyncio.Event()\n\n    self.task_states_map = self.backend.get_task_states_map()\n\n    # Setup working directory\n    self.work_dir = self.backend.session.path or os.getcwd()\n\n    # Register callback with backend\n    self.backend.register_callback(self.task_callbacks)\n\n    # Define decorators\n    self.block = self._register_decorator(comp_type=BLOCK)\n    self.function_task = self._register_decorator(\n        comp_type=TASK, task_type=FUNCTION\n    )\n    self.executable_task = self._register_decorator(\n        comp_type=TASK, task_type=EXECUTABLE\n    )\n\n    # Initialize async task references (will be set in _start_async_components)\n    self._run_task = None\n    self._shutdown_event = asyncio.Event()  # Added shutdown signal\n\n    self._setup_signal_handlers()\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine.create","title":"create  <code>async</code> <code>classmethod</code>","text":"<pre><code>create(backend: Optional[BaseExecutionBackend] = None, dry_run: bool = False, implicit_data: bool = True) -&gt; 'WorkflowEngine'\n</code></pre> <p>Factory method to create and initialize a WorkflowEngine.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Optional[BaseExecutionBackend]</code> <p>Execution backend. If None and dry_run=True,      uses NoopExecutionBackend</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>Whether to run in dry-run mode</p> <code>False</code> <code>implicit_data</code> <code>bool</code> <p>Whether to enable implicit data dependency linking</p> <code>True</code> <p>Returns:</p> Name Type Description <code>WorkflowEngine</code> <code>'WorkflowEngine'</code> <p>Fully initialized workflow engine</p> Example <p>engine = await WorkflowEngine.create(dry_run=True)</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>@classmethod\nasync def create(\n    cls,\n    backend: Optional[BaseExecutionBackend] = None,\n    dry_run: bool = False,\n    implicit_data: bool = True,\n) -&gt; \"WorkflowEngine\":\n    \"\"\"\n    Factory method to create and initialize a WorkflowEngine.\n\n    Args:\n        backend: Execution backend. If None and dry_run=True,\n                 uses NoopExecutionBackend\n        dry_run: Whether to run in dry-run mode\n        implicit_data: Whether to enable implicit data dependency linking\n\n    Returns:\n        WorkflowEngine: Fully initialized workflow engine\n\n    Example:\n        engine = await WorkflowEngine.create(dry_run=True)\n    \"\"\"\n    # Setup and validate backend first\n    validated_backend = cls._setup_execution_backend(backend, dry_run)\n\n    # Create instance with validated backend\n    instance = cls(\n        backend=validated_backend, dry_run=dry_run, implicit_data=implicit_data\n    )\n\n    # Initialize async components\n    await instance._start_async_components()\n\n    return instance\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine.run","title":"run  <code>async</code>","text":"<pre><code>run()\n</code></pre> <p>Manages asynchronous execution of workflow components.</p> <p>Continuously monitors and manages workflow components, handling their dependencies and execution states. Performs dependency resolution and prepares components for execution when their dependencies are satisfied.</p> Workflow Process <ol> <li>Monitors unresolved components</li> <li>Checks dependency resolution status</li> <li>Prepares resolved components for execution</li> <li>Handles data staging between components</li> <li>Submits ready components to execution</li> </ol> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>CancelledError</code> <p>If the coroutine is cancelled during execution</p> State Management <ul> <li>unresolved (set): Component UIDs with pending dependencies</li> <li>resolved (set): Component UIDs with satisfied dependencies</li> <li>running (list): Currently executing component UIDs</li> <li>dependencies (dict): Maps component UIDs to dependency info</li> <li>components (dict): Maps UIDs to component descriptions and futures</li> <li>queue (asyncio.Queue): Execution queue for ready components</li> </ul> Note <ul> <li>Runs indefinitely until cancelled or shutdown is signaled</li> <li>Uses sleep intervals to prevent busy-waiting</li> <li>Handles both implicit and explicit data dependencies</li> <li>Trigger internal shutdown on loop failure</li> </ul> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>async def run(self):\n    \"\"\"Manages asynchronous execution of workflow components.\n\n    Continuously monitors and manages workflow components, handling their\n    dependencies and execution states. Performs dependency resolution and\n    prepares components for execution when their dependencies are satisfied.\n\n    Workflow Process:\n        1. Monitors unresolved components\n        2. Checks dependency resolution status\n        3. Prepares resolved components for execution\n        4. Handles data staging between components\n        5. Submits ready components to execution\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    Raises:\n        asyncio.CancelledError: If the coroutine is cancelled during execution\n\n    State Management:\n        - unresolved (set): Component UIDs with pending dependencies\n        - resolved (set): Component UIDs with satisfied dependencies\n        - running (list): Currently executing component UIDs\n        - dependencies (dict): Maps component UIDs to dependency info\n        - components (dict): Maps UIDs to component descriptions and futures\n        - queue (asyncio.Queue): Execution queue for ready components\n\n    Note:\n        - Runs indefinitely until cancelled or shutdown is signaled\n        - Uses sleep intervals to prevent busy-waiting\n        - Handles both implicit and explicit data dependencies\n        - Trigger internal shutdown on loop failure\n    \"\"\"\n    while not self._shutdown_event.is_set():\n        try:\n            to_submit = []\n\n            # Process ready components\n            while self._ready_queue and not self._shutdown_event.is_set():\n                comp_uid = self._ready_queue.popleft()\n\n                # Skip if already processed\n                if comp_uid in self.resolved or comp_uid in self.running:\n                    continue\n\n                # Check if future is already done (could be cancelled/failed)\n                if self.components[comp_uid][\"future\"].done():\n                    self.resolved.add(comp_uid)\n                    self._notify_dependents(comp_uid)\n                    continue\n\n                # Verify dependencies are still met\n                dependencies = self.dependencies[comp_uid]\n                dep_futures = [\n                    self.components[dep[\"uid\"]][\"future\"] for dep in dependencies\n                ]\n\n                failed_deps = []\n                cancelled_deps = []\n\n                for fut in dep_futures:\n                    if fut.cancelled():\n                        cancelled_deps.append(fut)\n                    elif fut.exception() is not None:\n                        failed_deps.append(fut.exception())\n\n                # Handle dependency issues\n                if cancelled_deps or failed_deps:\n                    comp_desc = self.components[comp_uid][\"description\"]\n\n                    if cancelled_deps:\n                        logger.info(\n                            f\"Cancelling {comp_desc['name']} \"\n                            \"due to cancelled dependencies\"\n                        )\n                        self.handle_task_cancellation(\n                            comp_desc, self.components[comp_uid][\"future\"]\n                        )\n                    else:  # failed_deps\n                        chained_exception = (\n                            self._create_dependency_failure_exception(\n                                comp_desc, failed_deps\n                            )\n                        )\n                        logger.error(\n                            f\"Dependency failure for {comp_desc['name']}: \"\n                            f\"{chained_exception}\"\n                        )\n                        self.handle_task_failure(\n                            comp_desc,\n                            self.components[comp_uid][\"future\"],\n                            chained_exception,\n                        )\n\n                    # Common cleanup\n                    self.resolved.add(comp_uid)\n                    self._notify_dependents(comp_uid)\n                    continue\n\n                # Handle data dependencies for tasks\n                comp_desc = self.components[comp_uid][\"description\"]\n                if self.components[comp_uid][\"type\"] == TASK:\n                    explicit_files_to_stage = []\n\n                    for dep in dependencies:\n                        dep_desc = self.components[dep[\"uid\"]][\"description\"]\n\n                        # Link implicit data dependencies\n                        if self.implicit_data_mode and not dep_desc[\"metadata\"].get(\n                            \"output_files\"\n                        ):\n                            logger.debug(\n                                f\"Linking implicit file(s): from {dep_desc['name']} \"\n                                f\"to {comp_desc['name']}\"\n                            )\n                            self.backend.link_implicit_data_deps(\n                                dep_desc, comp_desc\n                            )\n\n                        # Link explicit data dependencies\n                        for output_file in dep_desc[\"metadata\"][\"output_files\"]:\n                            if output_file in comp_desc[\"metadata\"][\"input_files\"]:\n                                logger.debug(\n                                    f\"Linking explicit file ({output_file}) \"\n                                    f\"from {dep_desc['name']} \"\n                                    f\"to {comp_desc['name']}\"\n                                )\n                                data_dep = self.backend.link_explicit_data_deps(\n                                    src_task=dep_desc,\n                                    dst_task=comp_desc,\n                                    file_name=output_file,\n                                )\n                                explicit_files_to_stage.append(data_dep)\n\n                    # Input staging data dependencies\n                    dependency_output_files = self._get_dependency_output_files(\n                        dependencies\n                    )\n                    staged_targets = {\n                        Path(item[\"target\"]).name\n                        for item in explicit_files_to_stage\n                    }\n\n                    for input_file in comp_desc[\"metadata\"][\"input_files\"]:\n                        input_basename = Path(input_file).name\n                        if (\n                            input_basename not in staged_targets\n                            and input_basename not in dependency_output_files\n                        ):\n                            logger.debug(\n                                f\"Staging {input_file} \"\n                                f\"to {comp_desc['name']} work dir\"\n                            )\n                            data_dep = self.backend.link_explicit_data_deps(\n                                src_task=None,\n                                dst_task=comp_desc,\n                                file_name=input_basename,\n                                file_path=input_file,\n                            )\n                            explicit_files_to_stage.append(data_dep)\n\n                try:\n                    # Update the component description with resolved values\n                    (\n                        comp_desc[\"args\"],\n                        comp_desc[\"kwargs\"],\n                    ) = await self._extract_dependency_values(comp_desc)\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to resolve future for {comp_desc['name']}: {e}\"\n                    )\n                    self.handle_task_failure(\n                        comp_desc, self.components[comp_uid][\"future\"], e\n                    )\n                    self.resolved.add(comp_uid)\n                    self._notify_dependents(comp_uid)\n                    continue\n\n                to_submit.append(comp_desc)\n                res_deps = [dep[\"name\"] for dep in dependencies]\n                msg = f\"Ready to submit: {comp_desc['name']}\"\n                msg += f\" with resolved dependencies: {res_deps}\"\n                logger.debug(msg)\n\n            # Submit ready components\n            if to_submit:\n                await self.submit(to_submit)\n                for comp_desc in to_submit:\n                    comp_uid = comp_desc[\"uid\"]\n                    self.running.append(comp_uid)\n                    self.resolved.add(comp_uid)\n\n            # Check for completed components and update dependency tracking\n            completed_components = []\n            for comp_uid in list(self.running):\n                if self.components[comp_uid][\"future\"].done():\n                    completed_components.append(comp_uid)\n                    self.running.remove(comp_uid)\n\n            # Notify dependents of completed components\n            for comp_uid in completed_components:\n                self._notify_dependents(comp_uid)\n\n            # Signal changes\n            if completed_components:\n                self._component_change_event.set()\n\n            # If nothing is ready/running, wait for changes or shutdown\n            if not self._ready_queue and not to_submit and not completed_components:\n                try:\n                    # Create tasks for event waiting\n                    event_task = asyncio.create_task(\n                        self._component_change_event.wait(),\n                        name=\"component-event-task\",\n                    )\n                    shutdown_task = asyncio.create_task(\n                        self._shutdown_event.wait(), name=\"shutdown-event-task\"\n                    )\n\n                    done, pending = await asyncio.wait(\n                        [event_task, shutdown_task],\n                        return_when=asyncio.FIRST_COMPLETED,\n                        timeout=1.0,\n                    )\n                    # Cancel any pending tasks to clean up\n                    for task in pending:\n                        task.cancel()\n                    # Clear component change event if it was set\n                    if event_task in done:\n                        self._component_change_event.clear()\n                except asyncio.CancelledError:\n                    # If we get cancelled, make sure to clean up our tasks\n                    for task in [event_task, shutdown_task]:\n                        if not task.done():\n                            task.cancel()\n                    raise\n            else:\n                await asyncio.sleep(0.01)\n\n        except asyncio.CancelledError:\n            logger.debug(\"Run component stopped\")\n            break\n        except Exception as e:\n            logger.exception(f\"Error in run loop: {e}\")\n            await self._handle_shutdown_signal(signal.SIGUSR1, source=\"internal\")\n            break\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine.submit","title":"submit  <code>async</code>","text":"<pre><code>submit(objects)\n</code></pre> <p>Manages asynchronous submission of tasks/blocks to the execution backend.</p> <p>Retrieves and submit ready tasks and blocks for execution. Separates incoming objects into tasks and blocks based on their UID pattern, and dispatches each to the appropriate backend method.</p> Submission Process <ol> <li>Receive batch of objects</li> <li>Filters objects into tasks and blocks</li> <li>Submits tasks via <code>backend.submit_tasks</code></li> <li>Submits blocks via <code>_submit_blocks</code> asynchronously</li> </ol> <p>Parameters:</p> Name Type Description Default <code>objects</code> <p>Tasks and blocks are identified by <code>uid</code> field content</p> required <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an unexpected error occurs during submission</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>async def submit(self, objects):\n    \"\"\"Manages asynchronous submission of tasks/blocks to the execution backend.\n\n    Retrieves and submit ready tasks and blocks for execution. Separates incoming\n    objects into tasks and blocks based on their UID pattern, and dispatches each\n    to the appropriate backend method.\n\n    Submission Process:\n        1. Receive batch of objects\n        2. Filters objects into tasks and blocks\n        3. Submits tasks via `backend.submit_tasks`\n        4. Submits blocks via `_submit_blocks` asynchronously\n\n    Args:\n        objects: Tasks and blocks are identified by `uid` field content\n\n    Returns:\n        None\n\n    Raises:\n        Exception: If an unexpected error occurs during submission\n    \"\"\"\n    try:\n        # Separate tasks and blocks\n        tasks = [t for t in objects if t and BLOCK not in t[\"uid\"]]\n        blocks = [b for b in objects if b and TASK not in b[\"uid\"]]\n\n        logger.info(f\"Submitting {[b['name'] for b in objects]} for execution\")\n\n        if tasks:\n            await self.backend.submit_tasks(tasks)\n        if blocks:\n            await self._submit_blocks(blocks)\n    except Exception as e:\n        logger.exception(f\"Error in submit component: {e}\")\n        raise\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine.execute_block","title":"execute_block  <code>async</code>","text":"<pre><code>execute_block(block_fut: Future, func: Callable, *args, **kwargs)\n</code></pre> <p>Executes a block function and sets its result on the associated future.</p> <p>Calls the given function with provided args, awaiting it if it's a coroutine, or running it in the executor otherwise. On completion, updates the <code>block_fut</code> with the result or exception.</p> <p>Parameters:</p> Name Type Description Default <code>block_fut</code> <code>Future</code> <p>Future to store the result or exception.</p> required <code>func</code> <code>Callable</code> <p>Function or coroutine function to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>async def execute_block(\n    self, block_fut: asyncio.Future, func: Callable, *args, **kwargs\n):\n    \"\"\"Executes a block function and sets its result on the associated future.\n\n    Calls the given function with provided args, awaiting it if it's a coroutine,\n    or running it in the executor otherwise. On completion, updates the `block_fut`\n    with the result or exception.\n\n    Args:\n        block_fut (asyncio.Future): Future to store the result or exception.\n        func (Callable): Function or coroutine function to execute.\n        *args: Positional arguments to pass to the function.\n        **kwargs: Keyword arguments to pass to the function.\n\n    Returns:\n        None\n    \"\"\"\n\n    try:\n        if asyncio.iscoroutinefunction(func):\n            result = await func(*args, **kwargs)\n        else:\n            # Run sync function in executor\n            result = await self.loop.run_in_executor(None, func, *args, **kwargs)\n\n        if not block_fut.done():\n            block_fut.set_result(result)\n    except Exception as e:\n        if not block_fut.done():\n            block_fut.set_exception(e)\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine.handle_task_success","title":"handle_task_success","text":"<pre><code>handle_task_success(task: dict, task_fut: Future)\n</code></pre> <p>Handles successful task completion and updates the associated future.</p> <p>Sets the result of the task's future based on whether the task was a function or a shell command. Raises an error if the future is already resolved.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>Completed task descriptor containing - 'uid' (str): Unique task identifier - 'return_value' / 'stdout': Result of the task execution</p> required <code>task_fut</code> <code>Future</code> <p>Future to set the result on.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>def handle_task_success(self, task: dict, task_fut: asyncio.Future):\n    \"\"\"Handles successful task completion and updates the associated future.\n\n    Sets the result of the task's future based on whether the task was a function\n    or a shell command. Raises an error if the future is already resolved.\n\n    Args:\n        task (dict): Completed task descriptor containing\n            - 'uid' (str): Unique task identifier\n            - 'return_value' / 'stdout': Result of the task execution\n        task_fut (asyncio.Future): Future to set the result on.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    internal_task = self.components[task[\"uid\"]][\"description\"]\n\n    if not task_fut.done():\n        if internal_task[FUNCTION]:\n            task_fut.set_result(task[\"return_value\"])\n        else:\n            task_fut.set_result(task[\"stdout\"])\n    else:\n        logger.warning(\n            f'Attempted to handle an already finished task \"{task[\"uid\"]}\"'\n        )\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine.handle_task_failure","title":"handle_task_failure","text":"<pre><code>handle_task_failure(task: dict, task_fut: Future, override_error_message: Union[str, Exception] = None) -&gt; None\n</code></pre> <p>Handles task failure and sets the appropriate exception on the future.</p> <p>Marks the given task's future as failed by setting an exception derived from either a provided override error or the task's own recorded error/stderr. Logs a warning if the future is already resolved.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>Dictionary with task details, including: - 'uid' (str): Unique task identifier - 'exception' or 'stderr': Error information from execution</p> required <code>task_fut</code> <code>Union[SyncFuture, AsyncFuture]</code> <p>Future to mark as failed.</p> required <code>override_error_message</code> <code>Union[str, Exception]</code> <p>Custom</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>def handle_task_failure(\n    self,\n    task: dict,\n    task_fut: asyncio.Future,\n    override_error_message: Union[str, Exception] = None,\n) -&gt; None:\n    \"\"\"Handles task failure and sets the appropriate exception on the future.\n\n    Marks the given task's future as failed by setting an exception derived from\n    either a provided override error or the task's own recorded error/stderr. Logs\n    a warning if the future is already resolved.\n\n    Args:\n        task (dict): Dictionary with task details, including:\n            - 'uid' (str): Unique task identifier\n            - 'exception' or 'stderr': Error information from execution\n        task_fut (Union[SyncFuture, AsyncFuture]): Future to mark as failed.\n        override_error_message (Union[str, Exception], optional): Custom\n        error message or exception to set instead of the task's recorded error.\n\n    Returns:\n        None\n    \"\"\"\n    if task_fut.done():\n        logger.warning(\n            f'Attempted to handle an already failed task \"{task[\"uid\"]}\"'\n        )\n        return\n\n    # Determine the appropriate exception to set\n    if override_error_message is not None:\n        # If it's already an exception (like DependencyFailureError),\n        # use it directly\n        if isinstance(override_error_message, Exception):\n            exception = override_error_message\n        else:\n            # If it's a string, wrap it in RuntimeError\n            exception = RuntimeError(str(override_error_message))\n    else:\n        # Use the task's original exception or stderr\n        original_error = (\n            task.get(\"exception\")\n            or task.get(\"stderr\")\n            or \"failed with unknown error\"\n        )\n\n        # Ensure we have an Exception object\n        if isinstance(original_error, Exception):\n            exception = original_error\n        else:\n            # If it's a string (stderr) or any other type, wrap it in RuntimeError\n            exception = RuntimeError(str(original_error))\n\n    task_fut.set_exception(exception)\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine.handle_task_cancellation","title":"handle_task_cancellation","text":"<pre><code>handle_task_cancellation(task: dict, task_fut: Future)\n</code></pre> <p>Handle task cancellation.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>def handle_task_cancellation(self, task: dict, task_fut: asyncio.Future):\n    \"\"\"Handle task cancellation.\"\"\"\n    if task_fut.done():\n        logger.warning(\n            f'Attempted to handle an already cancelled task \"{task[\"uid\"]}\"'\n        )\n        return\n\n    # Restore original cancel method\n    task_fut.cancel = task_fut.original_cancel\n    return task_fut.cancel()\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine.task_callbacks","title":"task_callbacks","text":"<pre><code>task_callbacks(task, state: str, service_callback: Optional[Callable] = None)\n</code></pre> <p>Processes task state changes and invokes appropriate handlers.</p> <p>Handles state transitions for tasks, updates their futures, and triggers relevant state-specific handlers. Supports optional service-specific callbacks for extended functionality.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Union[dict, object]</code> <p>Task object or dictionary containing task</p> required <code>state</code> <code>str</code> <p>New state of the task.</p> required <code>service_callback</code> <code>Optional[Callable]</code> <p>Callback function</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> State Transitions <ul> <li>DONE: Calls handle_task_success</li> <li>RUNNING: Marks future as running</li> <li>CANCELED: Cancels the future</li> <li>FAILED: Calls handle_task_failure</li> </ul> Logging <ul> <li>Debug: Non-relevant state received</li> <li>Info: Task state changes</li> <li>Warning: Unknown task received</li> </ul> Example <p>::</p> <pre><code>def service_ready_callback(future, task, state):\n    def wait_and_set():\n        try:\n            # Synchronous operation\n            future.set_result(info)\n        except Exception as e:\n            future.set_exception(e)\n    threading.Thread(target=wait_and_set, daemon=True).start()\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>@typeguard.typechecked\ndef task_callbacks(\n    self, task, state: str, service_callback: Optional[Callable] = None\n):\n    \"\"\"Processes task state changes and invokes appropriate handlers.\n\n    Handles state transitions for tasks, updates their futures, and triggers\n    relevant state-specific handlers. Supports optional service-specific\n    callbacks for extended functionality.\n\n    Args:\n        task (Union[dict, object]): Task object or dictionary containing task\n        state information.\n        state (str): New state of the task.\n        service_callback (Optional[Callable], optional): Callback function\n        for service tasks. Must be daemon-threaded to avoid blocking.\n        Defaults to None.\n\n    Returns:\n        None\n\n    State Transitions:\n        - DONE: Calls handle_task_success\n        - RUNNING: Marks future as running\n        - CANCELED: Cancels the future\n        - FAILED: Calls handle_task_failure\n\n    Logging:\n        - Debug: Non-relevant state received\n        - Info: Task state changes\n        - Warning: Unknown task received\n\n    Example:\n        ::\n\n            def service_ready_callback(future, task, state):\n                def wait_and_set():\n                    try:\n                        # Synchronous operation\n                        future.set_result(info)\n                    except Exception as e:\n                        future.set_exception(e)\n                threading.Thread(target=wait_and_set, daemon=True).start()\n    \"\"\"\n    if (\n        state not in self.task_states_map.terminal_states\n        and state != self.task_states_map.RUNNING\n    ):\n        logger.debug(f\"Non-relevant task state received: {state}. Skipping state.\")\n        return\n\n    task_obj = task\n    if isinstance(task, dict):\n        task_dct = task\n    else:\n        task_dct = task.as_dict()\n\n    if task_dct[\"uid\"] not in self.components:\n        logger.warning(\n            f\"Received an unknown task and will skip it: {task_dct['uid']}\"\n        )\n        return\n\n    task_fut = self.components[task_dct[\"uid\"]][\"future\"]\n    logger.info(f\"{task_dct['uid']} is in {state} state\")\n\n    if service_callback:\n        # service tasks are marked done by a backend specific\n        # mechanism that are provided during the callbacks only\n        service_callback(task_fut, task_obj, state)\n\n    if state == self.task_states_map.DONE:\n        self.handle_task_success(task_dct, task_fut)\n    elif state == self.task_states_map.RUNNING:\n        # NOTE: with asyncio future the running state is\n        # implicit: when a coroutine that awaits the future\n        # is scheduled and started by the event loop, that\u2019s\n        # when the \u201cwork\u201d is running.\n        pass\n    elif state == self.task_states_map.CANCELED:\n        self.handle_task_cancellation(task_dct, task_fut)\n    elif state == self.task_states_map.FAILED:\n        self.handle_task_failure(task_dct, task_fut)\n</code></pre>"},{"location":"api/radical/asyncflow/workflow_manager/#radical.asyncflow.workflow_manager.WorkflowEngine.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown(skip_execution_backend: bool = False)\n</code></pre> <p>Internal implementation of asynchronous shutdown for the workflow manager.</p> This method performs the following steps <ol> <li>Sets the shutdown event to signal components to exit</li> <li>Cancels background tasks responsible for running and submitting workflows.</li> <li>Waits for the cancellation and completion of these tasks, with a timeout of 5 seconds.</li> <li>Cancel workflow tasks.</li> <li>Logs a warning if the tasks do not complete within the timeout period.</li> <li>Shuts down the backend using an executor to avoid blocking the event loop.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>skip_execution_backend</code> <code>bool</code> <p>If True, skips the shutdown of the execution backend.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the background tasks do not complete within the timeout period.</p> <code>CancelledError</code> <p>If the shutdown is cancelled before completion.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/workflow_manager.py</code> <pre><code>async def shutdown(self, skip_execution_backend: bool = False):\n    \"\"\"\n    Internal implementation of asynchronous shutdown for the workflow manager.\n\n    This method performs the following steps:\n        1. Sets the shutdown event to signal components to exit\n        2. Cancels background tasks responsible for running and\n        submitting workflows.\n        3. Waits for the cancellation and completion of these tasks,\n        with a timeout of 5 seconds.\n        4. Cancel workflow tasks.\n        5. Logs a warning if the tasks do not complete within the timeout\n        period.\n        6. Shuts down the backend using an executor to avoid blocking the\n        event loop.\n\n    Args:\n        skip_execution_backend (bool): If True, skips the shutdown of the\n            execution backend.\n\n    Returns:\n        None\n\n    Raises:\n        asyncio.TimeoutError: If the background tasks do not complete\n            within the timeout period.\n        asyncio.CancelledError: If the shutdown is cancelled before\n            completion.\n    \"\"\"\n    logger.info(\"Initiating shutdown\")\n    # Signal components to exit\n    self._shutdown_event.set()\n\n    # cancel workflow futures (tasks and blocks)\n    for comp in self.components.values():\n        future = comp[\"future\"]\n        comp_desc = comp[\"description\"]\n        if not future.done():\n            self.handle_task_cancellation(comp_desc, future)\n\n    # Cancel internal components task\n    if not self._run_task.done():\n        logger.debug(f\"Shutting down run component\")\n        self._run_task.cancel()\n\n    # Wait for internal components shutdown to complete\n    try:\n        await asyncio.wait_for(self._run_task, timeout=5.0)\n    except asyncio.TimeoutError:\n        logger.warning(\"Timeout waiting for internal components to shutdown\")\n    except asyncio.CancelledError:\n        logger.warning(\"Internal components shutdown cancelled\")\n\n    # Shutdown execution backend\n    if not skip_execution_backend and self.backend:\n        await self.backend.shutdown()\n        self._clear_internal_records()\n        logger.debug(\"Shutting down execution backend completed\")\n    else:\n        logger.warning(\"Skipping execution backend shutdown as requested\")\n\n    logger.info(\"Shutdown completed for all components.\")\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/","title":"base","text":""},{"location":"api/radical/asyncflow/backends/execution/base/#radicalasyncflowbackendsexecutionbase","title":"radical.asyncflow.backends.execution.base","text":""},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend","title":"BaseExecutionBackend","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for execution backends that manage task execution and state.</p> <p>This class defines the interface for execution backends that handle task submission, state management, and dependency linking in a distributed or parallel execution environment.</p>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.submit_tasks","title":"submit_tasks  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>submit_tasks(tasks: list[dict]) -&gt; None\n</code></pre> <p>Submit a list of tasks for execution.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>list[dict]</code> <p>A list of dictionaries containing task definitions and metadata. Each task dictionary should contain the necessary information for task execution.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\nasync def submit_tasks(self, tasks: list[dict]) -&gt; None:\n    \"\"\"Submit a list of tasks for execution.\n\n    Args:\n        tasks: A list of dictionaries containing task definitions and metadata.\n            Each task dictionary should contain the necessary information for\n            task execution.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.shutdown","title":"shutdown  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Gracefully shutdown the execution backend.</p> <p>This method should clean up resources, terminate running tasks if necessary, and prepare the backend for termination.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\nasync def shutdown(self) -&gt; None:\n    \"\"\"Gracefully shutdown the execution backend.\n\n    This method should clean up resources, terminate running tasks if necessary,\n    and prepare the backend for termination.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.state","title":"state  <code>abstractmethod</code>","text":"<pre><code>state() -&gt; str\n</code></pre> <p>Get the current state of the execution backend.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representing the current state of the backend (e.g., 'running',</p> <code>str</code> <p>'idle', 'shutting_down', 'error').</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\ndef state(self) -&gt; str:\n    \"\"\"Get the current state of the execution backend.\n\n    Returns:\n        A string representing the current state of the backend (e.g., 'running',\n        'idle', 'shutting_down', 'error').\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.task_state_cb","title":"task_state_cb  <code>abstractmethod</code>","text":"<pre><code>task_state_cb(task: dict, state: str) -&gt; None\n</code></pre> <p>Callback function invoked when a task's state changes.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>Dictionary containing task information and metadata.</p> required <code>state</code> <code>str</code> <p>The new state of the task (e.g., 'pending', 'running', 'completed', 'failed').</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\ndef task_state_cb(self, task: dict, state: str) -&gt; None:\n    \"\"\"Callback function invoked when a task's state changes.\n\n    Args:\n        task: Dictionary containing task information and metadata.\n        state: The new state of the task (e.g., 'pending', 'running', 'completed',\n            'failed').\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.register_callback","title":"register_callback  <code>abstractmethod</code>","text":"<pre><code>register_callback(func) -&gt; None\n</code></pre> <p>Register a callback function for task state changes.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>A callable that will be invoked when task states change. The function should accept task and state parameters.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\ndef register_callback(self, func) -&gt; None:\n    \"\"\"Register a callback function for task state changes.\n\n    Args:\n        func: A callable that will be invoked when task states change.\n            The function should accept task and state parameters.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.get_task_states_map","title":"get_task_states_map  <code>abstractmethod</code>","text":"<pre><code>get_task_states_map() -&gt; None\n</code></pre> <p>Retrieve a mapping of task IDs to their current states.</p> <p>Returns:</p> Type Description <code>None</code> <p>A dictionary mapping task identifiers to their current execution states.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\ndef get_task_states_map(self) -&gt; None:\n    \"\"\"Retrieve a mapping of task IDs to their current states.\n\n    Returns:\n        A dictionary mapping task identifiers to their current execution states.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.build_task","title":"build_task  <code>abstractmethod</code>","text":"<pre><code>build_task(task: dict) -&gt; None\n</code></pre> <p>Build or prepare a task for execution.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>Dictionary containing task definition, parameters, and metadata required for task construction.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\ndef build_task(self, task: dict) -&gt; None:\n    \"\"\"Build or prepare a task for execution.\n\n    Args:\n        task: Dictionary containing task definition, parameters, and metadata\n            required for task construction.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.link_implicit_data_deps","title":"link_implicit_data_deps  <code>abstractmethod</code>","text":"<pre><code>link_implicit_data_deps(src_task, dst_task)\n</code></pre> <p>Link implicit data dependencies between two tasks.</p> <p>Creates a dependency relationship where the destination task depends on data produced by the source task, with the dependency being inferred automatically.</p> <p>Parameters:</p> Name Type Description Default <code>src_task</code> <p>The source task that produces data.</p> required <code>dst_task</code> <p>The destination task that depends on the source task's output.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\ndef link_implicit_data_deps(self, src_task, dst_task):\n    \"\"\"Link implicit data dependencies between two tasks.\n\n    Creates a dependency relationship where the destination task depends on\n    data produced by the source task, with the dependency being inferred\n    automatically.\n\n    Args:\n        src_task: The source task that produces data.\n        dst_task: The destination task that depends on the source task's output.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.link_explicit_data_deps","title":"link_explicit_data_deps  <code>abstractmethod</code>","text":"<pre><code>link_explicit_data_deps(src_task=None, dst_task=None, file_name=None, file_path=None)\n</code></pre> <p>Link explicit data dependencies between tasks or files.</p> <p>Creates explicit dependency relationships based on specified file names or paths, allowing for more precise control over task execution order.</p> <p>Parameters:</p> Name Type Description Default <code>src_task</code> <p>The source task that produces the dependency.</p> <code>None</code> <code>dst_task</code> <p>The destination task that depends on the source.</p> <code>None</code> <code>file_name</code> <p>Name of the file that represents the dependency.</p> <code>None</code> <code>file_path</code> <p>Full path to the file that represents the dependency.</p> <code>None</code> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\ndef link_explicit_data_deps(\n    self, src_task=None, dst_task=None, file_name=None, file_path=None\n):\n    \"\"\"Link explicit data dependencies between tasks or files.\n\n    Creates explicit dependency relationships based on specified file names\n    or paths, allowing for more precise control over task execution order.\n\n    Args:\n        src_task: The source task that produces the dependency.\n        dst_task: The destination task that depends on the source.\n        file_name: Name of the file that represents the dependency.\n        file_path: Full path to the file that represents the dependency.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.BaseExecutionBackend.cancel_task","title":"cancel_task  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>cancel_task(uid: str) -&gt; bool\n</code></pre> <p>Cancel a task in the execution backend.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Task identifier</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the backend doesn't support cancellation</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>@abstractmethod\nasync def cancel_task(self, uid: str) -&gt; bool:\n    \"\"\"\n    Cancel a task in the execution backend.\n\n    Args:\n        uid: Task identifier\n\n    Raises:\n        NotImplementedError: If the backend doesn't support cancellation\n    \"\"\"\n    raise NotImplementedError(\"Not implemented in the base backend\")\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/base/#radical.asyncflow.backends.execution.base.Session","title":"Session","text":"<pre><code>Session()\n</code></pre> <p>Manages execution session state and working directory.</p> <p>This class maintains session-specific information including the current working directory path for task execution.</p> <p>Initialize a new session with the current working directory.</p> <p>Sets the session path to the current working directory at the time of initialization.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize a new session with the current working directory.\n\n    Sets the session path to the current working directory at the time\n    of initialization.\n    \"\"\"\n    self.path = os.getcwd()\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/concurrent/","title":"concurrent","text":""},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radicalasyncflowbackendsexecutionconcurrent","title":"radical.asyncflow.backends.execution.concurrent","text":""},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radical.asyncflow.backends.execution.concurrent.ConcurrentExecutionBackend","title":"ConcurrentExecutionBackend","text":"<pre><code>ConcurrentExecutionBackend(executor: Executor)\n</code></pre> <p>               Bases: <code>BaseExecutionBackend</code></p> <p>Simple async-only concurrent execution backend.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/concurrent.py</code> <pre><code>def __init__(self, executor: Executor):\n    if not isinstance(executor, Executor):\n        err = \"Executor must be ThreadPoolExecutor or ProcessPoolExecutor\"\n        raise TypeError(err)\n\n    if isinstance(executor, ProcessPoolExecutor) and cloudpickle is None:\n        raise ImportError(\n            \"ProcessPoolExecutor requires 'cloudpickle'. \"\n            \"Install it with: pip install cloudpickle\"\n        )\n\n    self.executor = executor\n    self.tasks: dict[str, asyncio.Task] = {}\n    self.session = Session()\n    self._callback_func: Optional[Callable] = None\n    self._initialized = False\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radical.asyncflow.backends.execution.concurrent.ConcurrentExecutionBackend.__await__","title":"__await__","text":"<pre><code>__await__()\n</code></pre> <p>Make backend awaitable.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/concurrent.py</code> <pre><code>def __await__(self):\n    \"\"\"Make backend awaitable.\"\"\"\n    return self._async_init().__await__()\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radical.asyncflow.backends.execution.concurrent.ConcurrentExecutionBackend.submit_tasks","title":"submit_tasks  <code>async</code>","text":"<pre><code>submit_tasks(tasks: list[dict[str, Any]]) -&gt; list[Task]\n</code></pre> <p>Submit tasks for execution.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/concurrent.py</code> <pre><code>async def submit_tasks(self, tasks: list[dict[str, Any]]) -&gt; list[asyncio.Task]:\n    \"\"\"Submit tasks for execution.\"\"\"\n    submitted_tasks = []\n\n    for task in tasks:\n        future = asyncio.create_task(self._handle_task(task))\n        submitted_tasks.append(future)\n\n        self.tasks[task[\"uid\"]] = task\n        self.tasks[task[\"uid\"]][\"future\"] = future\n\n    return submitted_tasks\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radical.asyncflow.backends.execution.concurrent.ConcurrentExecutionBackend.cancel_task","title":"cancel_task  <code>async</code>","text":"<pre><code>cancel_task(uid: str) -&gt; bool\n</code></pre> <p>Cancel a task by its UID.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>The UID of the task to cancel.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the task was found and cancellation was attempted,   False otherwise.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/concurrent.py</code> <pre><code>async def cancel_task(self, uid: str) -&gt; bool:\n    \"\"\"Cancel a task by its UID.\n\n    Args:\n        uid (str): The UID of the task to cancel.\n\n    Returns:\n        bool: True if the task was found and cancellation was attempted,\n              False otherwise.\n    \"\"\"\n    if uid in self.tasks:\n        task = self.tasks[uid]\n        future = task[\"future\"]\n        if future and future.cancel():\n            self._callback_func(task, \"CANCELED\")\n            return True\n    return False\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radical.asyncflow.backends.execution.concurrent.ConcurrentExecutionBackend.cancel_all_tasks","title":"cancel_all_tasks  <code>async</code>","text":"<pre><code>cancel_all_tasks() -&gt; int\n</code></pre> <p>Cancel all running tasks.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/concurrent.py</code> <pre><code>async def cancel_all_tasks(self) -&gt; int:\n    \"\"\"Cancel all running tasks.\"\"\"\n    cancelled_count = 0\n    for task in self.tasks.values():\n        future = task[\"future\"]\n        future.cancel()\n        cancelled_count += 1\n    self.tasks.clear()\n    return cancelled_count\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radical.asyncflow.backends.execution.concurrent.ConcurrentExecutionBackend.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown the executor.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/concurrent.py</code> <pre><code>async def shutdown(self) -&gt; None:\n    \"\"\"Shutdown the executor.\"\"\"\n    await self.cancel_all_tasks()\n    self.executor.shutdown(wait=True)\n    logger.info(\"Concurrent execution backend shutdown complete\")\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radical.asyncflow.backends.execution.concurrent.ConcurrentExecutionBackend.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__()\n</code></pre> <p>Async context manager entry.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/concurrent.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Async context manager entry.\"\"\"\n    if not self._initialized:\n        await self._async_init()\n    return self\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radical.asyncflow.backends.execution.concurrent.ConcurrentExecutionBackend.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Async context manager exit.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/concurrent.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Async context manager exit.\"\"\"\n    await self.shutdown()\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/concurrent/#radical.asyncflow.backends.execution.concurrent.ConcurrentExecutionBackend.create","title":"create  <code>async</code> <code>classmethod</code>","text":"<pre><code>create(executor: Executor)\n</code></pre> <p>Alternative factory method for creating initialized backend.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Executor</code> <p>A concurrent.Executor instance (ThreadPoolExecutor       or ProcessPoolExecutor).</p> required <p>Returns:</p> Type Description <p>Fully initialized ConcurrentExecutionBackend instance.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/concurrent.py</code> <pre><code>@classmethod\nasync def create(cls, executor: Executor):\n    \"\"\"Alternative factory method for creating initialized backend.\n\n    Args:\n        executor: A concurrent.Executor instance (ThreadPoolExecutor\n                  or ProcessPoolExecutor).\n\n    Returns:\n        Fully initialized ConcurrentExecutionBackend instance.\n    \"\"\"\n    backend = cls(executor)\n    return await backend\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/","title":"dask_parallel","text":""},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radicalasyncflowbackendsexecutiondask_parallel","title":"radical.asyncflow.backends.execution.dask_parallel","text":""},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend","title":"DaskExecutionBackend","text":"<pre><code>DaskExecutionBackend(resources: Optional[dict] = None)\n</code></pre> <p>               Bases: <code>BaseExecutionBackend</code></p> <p>An async-only Dask execution backend for distributed task execution.</p> <p>Handles task submission, cancellation, and proper async event loop handling for distributed task execution using Dask. All functions must be async.</p> Usage <p>backend = await DaskExecutionBackend(resources)</p> <p>Initialize the Dask execution backend (non-async setup only).</p> <p>Parameters:</p> Name Type Description Default <code>resources</code> <code>Optional[dict]</code> <p>Dictionary of resource requirements for tasks. Contains configuration parameters for the Dask client initialization.</p> <code>None</code> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>@typeguard.typechecked\ndef __init__(self, resources: Optional[dict] = None):\n    \"\"\"Initialize the Dask execution backend (non-async setup only).\n\n    Args:\n        resources: Dictionary of resource requirements for tasks. Contains\n            configuration parameters for the Dask client initialization.\n    \"\"\"\n\n    if dask is None:\n        raise ImportError(\"Dask is required for DaskExecutionBackend.\")\n\n    self.tasks = {}\n    self._client = None\n    self.session = Session()\n    self._callback_func = None\n    self._resources = resources or {}\n    self._initialized = False\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend--or","title":"or","text":"<p>async with DaskExecutionBackend(resources) as backend:     await backend.submit_tasks(tasks)</p>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.__await__","title":"__await__","text":"<pre><code>__await__()\n</code></pre> <p>Make DaskExecutionBackend awaitable like Dask Client.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>def __await__(self):\n    \"\"\"Make DaskExecutionBackend awaitable like Dask Client.\"\"\"\n    return self._async_init().__await__()\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.register_callback","title":"register_callback","text":"<pre><code>register_callback(callback: Callable) -&gt; None\n</code></pre> <p>Register a callback for task state changes.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable</code> <p>Function to be called when task states change. Should accept task and state parameters.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>def register_callback(self, callback: Callable) -&gt; None:\n    \"\"\"Register a callback for task state changes.\n\n    Args:\n        callback: Function to be called when task states change. Should accept\n            task and state parameters.\n    \"\"\"\n    self._callback_func = callback\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.get_task_states_map","title":"get_task_states_map","text":"<pre><code>get_task_states_map()\n</code></pre> <p>Retrieve a mapping of task IDs to their current states.</p> <p>Returns:</p> Name Type Description <code>StateMapper</code> <p>Object containing the mapping of task states for this backend.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>def get_task_states_map(self):\n    \"\"\"Retrieve a mapping of task IDs to their current states.\n\n    Returns:\n        StateMapper: Object containing the mapping of task states for this backend.\n    \"\"\"\n    return StateMapper(backend=self)\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.cancel_task","title":"cancel_task  <code>async</code>","text":"<pre><code>cancel_task(uid: str) -&gt; bool\n</code></pre> <p>Cancel a task by its UID.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>The UID of the task to cancel.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the task was found and cancellation was attempted,</p> <code>bool</code> <p>False otherwise.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>async def cancel_task(self, uid: str) -&gt; bool:\n    \"\"\"Cancel a task by its UID.\n\n    Args:\n        uid (str): The UID of the task to cancel.\n\n    Returns:\n        bool: True if the task was found and cancellation was attempted,\n        False otherwise.\n    \"\"\"\n    self._ensure_initialized()\n    if uid in self.tasks:\n        task = self.tasks[uid]\n        future = task.get(\"future\")\n        if future:\n            return await future.cancel()\n    return False\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.submit_tasks","title":"submit_tasks  <code>async</code>","text":"<pre><code>submit_tasks(tasks: list[dict[str, Any]]) -&gt; None\n</code></pre> <p>Submit async tasks to Dask cluster.</p> <p>Processes a list of tasks and submits them to the Dask cluster for execution. Filters out future objects from arguments and validates that all functions are async coroutine functions.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>list[dict[str, Any]]</code> <p>List of task dictionaries containing: - uid: Unique task identifier - function: Async callable to execute - args: Positional arguments - kwargs: Keyword arguments - executable: Optional executable path (not supported) - task_backend_specific_kwargs: Backend-specific parameters</p> required Note <p>Executable tasks are not supported and will result in FAILED state. Only async functions are supported - sync functions will result in FAILED state. Future objects are filtered out from arguments as they are not picklable.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>async def submit_tasks(self, tasks: list[dict[str, Any]]) -&gt; None:\n    \"\"\"Submit async tasks to Dask cluster.\n\n    Processes a list of tasks and submits them to the Dask cluster for execution.\n    Filters out future objects from arguments and validates that all functions\n    are async coroutine functions.\n\n    Args:\n        tasks: List of task dictionaries containing:\n            - uid: Unique task identifier\n            - function: Async callable to execute\n            - args: Positional arguments\n            - kwargs: Keyword arguments\n            - executable: Optional executable path (not supported)\n            - task_backend_specific_kwargs: Backend-specific parameters\n\n    Note:\n        Executable tasks are not supported and will result in FAILED state.\n        Only async functions are supported - sync functions will result in\n        FAILED state.\n        Future objects are filtered out from arguments as they are not picklable.\n    \"\"\"\n    self._ensure_initialized()\n\n    for task in tasks:\n        is_func_task = bool(task.get(\"function\"))\n        is_exec_task = bool(task.get(\"executable\"))\n\n        if is_exec_task:\n            error_msg = \"DaskExecutionBackend does not support executable tasks\"\n            task[\"stderr\"] = ValueError(error_msg)\n            self._callback_func(task, \"FAILED\")\n            continue\n\n        # Validate that function is async\n        if is_func_task and not asyncio.iscoroutinefunction(task[\"function\"]):\n            error_msg = \"DaskExecutionBackend only supports async functions\"\n            task[\"exception\"] = ValueError(error_msg)\n            self._callback_func(task, \"FAILED\")\n            continue\n\n        self.tasks[task[\"uid\"]] = task\n\n        # Filter out future objects as they are not picklable\n        filtered_args = [\n            arg for arg in task[\"args\"] if not isinstance(arg, asyncio.Future)\n        ]\n        task[\"args\"] = tuple(filtered_args)\n        try:\n            await self._submit_async_function(task)\n        except Exception as e:\n            task[\"exception\"] = e\n            self._callback_func(task, \"FAILED\")\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.cancel_all_tasks","title":"cancel_all_tasks  <code>async</code>","text":"<pre><code>cancel_all_tasks() -&gt; int\n</code></pre> <p>Cancel all currently running/pending tasks.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of tasks that were successfully cancelled</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>async def cancel_all_tasks(self) -&gt; int:\n    \"\"\"Cancel all currently running/pending tasks.\n\n    Returns:\n        Number of tasks that were successfully cancelled\n    \"\"\"\n    self._ensure_initialized()\n    cancelled_count = 0\n    task_uids = list(self.tasks.keys())\n\n    for task_uid in task_uids:\n        if await self.cancel_task(task_uid):\n            cancelled_count += 1\n\n    return cancelled_count\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.link_explicit_data_deps","title":"link_explicit_data_deps","text":"<pre><code>link_explicit_data_deps(src_task=None, dst_task=None, file_name=None, file_path=None)\n</code></pre> <p>Handle explicit data dependencies between tasks.</p> <p>Parameters:</p> Name Type Description Default <code>src_task</code> <p>The source task that produces the dependency.</p> <code>None</code> <code>dst_task</code> <p>The destination task that depends on the source.</p> <code>None</code> <code>file_name</code> <p>Name of the file that represents the dependency.</p> <code>None</code> <code>file_path</code> <p>Full path to the file that represents the dependency.</p> <code>None</code> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>def link_explicit_data_deps(\n    self, src_task=None, dst_task=None, file_name=None, file_path=None\n):\n    \"\"\"Handle explicit data dependencies between tasks.\n\n    Args:\n        src_task: The source task that produces the dependency.\n        dst_task: The destination task that depends on the source.\n        file_name: Name of the file that represents the dependency.\n        file_path: Full path to the file that represents the dependency.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.link_implicit_data_deps","title":"link_implicit_data_deps","text":"<pre><code>link_implicit_data_deps(src_task, dst_task)\n</code></pre> <p>Handle implicit data dependencies for a task.</p> <p>Parameters:</p> Name Type Description Default <code>src_task</code> <p>The source task that produces data.</p> required <code>dst_task</code> <p>The destination task that depends on the source task's output.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>def link_implicit_data_deps(self, src_task, dst_task):\n    \"\"\"Handle implicit data dependencies for a task.\n\n    Args:\n        src_task: The source task that produces data.\n        dst_task: The destination task that depends on the source task's output.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.state","title":"state  <code>async</code>","text":"<pre><code>state() -&gt; str\n</code></pre> <p>Get the current state of the Dask execution backend.</p> <p>Returns:</p> Type Description <code>str</code> <p>Current state of the backend as a string.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>async def state(self) -&gt; str:\n    \"\"\"Get the current state of the Dask execution backend.\n\n    Returns:\n        Current state of the backend as a string.\n    \"\"\"\n    if not self._initialized or self._client is None:\n        return \"DISCONNECTED\"\n\n    try:\n        # Check if client is still connected\n        await self._client.scheduler_info()\n        return \"CONNECTED\"\n    except Exception:\n        return \"DISCONNECTED\"\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.task_state_cb","title":"task_state_cb  <code>async</code>","text":"<pre><code>task_state_cb(task: dict, state: str) -&gt; None\n</code></pre> <p>Callback function invoked when a task's state changes.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>Dictionary containing task information and metadata.</p> required <code>state</code> <code>str</code> <p>The new state of the task.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>async def task_state_cb(self, task: dict, state: str) -&gt; None:\n    \"\"\"Callback function invoked when a task's state changes.\n\n    Args:\n        task: Dictionary containing task information and metadata.\n        state: The new state of the task.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.build_task","title":"build_task  <code>async</code>","text":"<pre><code>build_task(task: dict) -&gt; None\n</code></pre> <p>Build or prepare a task for execution.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>Dictionary containing task definition, parameters, and metadata required for task construction.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>async def build_task(self, task: dict) -&gt; None:\n    \"\"\"Build or prepare a task for execution.\n\n    Args:\n        task: Dictionary containing task definition, parameters, and metadata\n            required for task construction.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown the Dask client and clean up resources.</p> <p>Closes the Dask client connection, clears task storage, and handles any cleanup exceptions gracefully.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>async def shutdown(self) -&gt; None:\n    \"\"\"Shutdown the Dask client and clean up resources.\n\n    Closes the Dask client connection, clears task storage, and handles\n    any cleanup exceptions gracefully.\n    \"\"\"\n    if self._client is not None:\n        try:\n            # Cancel all running tasks first\n            await self.cancel_all_tasks()\n\n            # Close the client\n            await self._client.close()\n            logger.info(\"Dask client shutdown complete\")\n        except Exception as e:\n            logger.exception(f\"Error during shutdown: {str(e)}\")\n        finally:\n            self._client = None\n            self.tasks.clear()\n            self._initialized = False\n            logger.info(\"Dask execution backend shutdown complete\")\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__()\n</code></pre> <p>Async context manager entry.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Async context manager entry.\"\"\"\n    if not self._initialized:\n        await self._async_init()\n    return self\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Async context manager exit.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Async context manager exit.\"\"\"\n    await self.shutdown()\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/dask_parallel/#radical.asyncflow.backends.execution.dask_parallel.DaskExecutionBackend.create","title":"create  <code>async</code> <code>classmethod</code>","text":"<pre><code>create(resources: Optional[dict] = None)\n</code></pre> <p>Alternative factory method for creating initialized backend.</p> <p>Parameters:</p> Name Type Description Default <code>resources</code> <code>Optional[dict]</code> <p>Configuration parameters for Dask client initialization.</p> <code>None</code> <p>Returns:</p> Type Description <p>Fully initialized DaskExecutionBackend instance.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/dask_parallel.py</code> <pre><code>@classmethod\nasync def create(cls, resources: Optional[dict] = None):\n    \"\"\"Alternative factory method for creating initialized backend.\n\n    Args:\n        resources: Configuration parameters for Dask client initialization.\n\n    Returns:\n        Fully initialized DaskExecutionBackend instance.\n    \"\"\"\n    backend = cls(resources)\n    return await backend\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/","title":"noop","text":""},{"location":"api/radical/asyncflow/backends/execution/noop/#radicalasyncflowbackendsexecutionnoop","title":"radical.asyncflow.backends.execution.noop","text":""},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend","title":"NoopExecutionBackend","text":"<pre><code>NoopExecutionBackend()\n</code></pre> <p>               Bases: <code>BaseExecutionBackend</code></p> <p>A no-operation execution backend for testing and development purposes.</p> <p>This backend simulates task execution without actually running any tasks. All submitted tasks immediately return dummy output and transition to DONE state. Useful for testing workflow logic without computational overhead.</p> <p>Initialize the no-op execution backend.</p> <p>Sets up dummy task storage, session, and default callback function. Registers backend states and confirms successful initialization.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the no-op execution backend.\n\n    Sets up dummy task storage, session, and default callback function.\n    Registers backend states and confirms successful initialization.\n    \"\"\"\n    self.tasks = {}\n    self.session = Session()\n    self._callback_func: Callable = lambda task, state: None  # default no-op\n    StateMapper.register_backend_states_with_defaults(backend=self)\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend.state","title":"state","text":"<pre><code>state()\n</code></pre> <p>Get the current state of the no-op execution backend.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>Always returns 'IDLE' as this backend performs no actual work.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>def state(self):\n    \"\"\"Get the current state of the no-op execution backend.\n\n    Returns:\n        str: Always returns 'IDLE' as this backend performs no actual work.\n    \"\"\"\n    return \"IDLE\"\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend.task_state_cb","title":"task_state_cb","text":"<pre><code>task_state_cb(task: dict, state: str) -&gt; None\n</code></pre> <p>Callback function invoked when a task's state changes.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>Dictionary containing task information and metadata.</p> required <code>state</code> <code>str</code> <p>The new state of the task.</p> required Note <p>This is a no-op implementation that performs no actions.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>def task_state_cb(self, task: dict, state: str) -&gt; None:\n    \"\"\"Callback function invoked when a task's state changes.\n\n    Args:\n        task: Dictionary containing task information and metadata.\n        state: The new state of the task.\n\n    Note:\n        This is a no-op implementation that performs no actions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend.get_task_states_map","title":"get_task_states_map","text":"<pre><code>get_task_states_map()\n</code></pre> <p>Retrieve a mapping of task IDs to their current states.</p> <p>Returns:</p> Name Type Description <code>StateMapper</code> <p>Object containing the mapping of task states for this backend.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>def get_task_states_map(self):\n    \"\"\"Retrieve a mapping of task IDs to their current states.\n\n    Returns:\n        StateMapper: Object containing the mapping of task states for this backend.\n    \"\"\"\n    return StateMapper(backend=self)\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend.register_callback","title":"register_callback","text":"<pre><code>register_callback(func: Callable)\n</code></pre> <p>Register a callback for task state changes.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to be called when task states change. Should accept task and state parameters.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>def register_callback(self, func: Callable):\n    \"\"\"Register a callback for task state changes.\n\n    Args:\n        func: Function to be called when task states change. Should accept\n            task and state parameters.\n    \"\"\"\n    self._callback_func = func\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend.build_task","title":"build_task","text":"<pre><code>build_task(uid, task_desc, task_specific_kwargs)\n</code></pre> <p>Build or prepare a task for execution.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <p>Unique identifier for the task.</p> required <code>task_desc</code> <p>Dictionary containing task description and metadata.</p> required <code>task_specific_kwargs</code> <p>Backend-specific keyword arguments for the task.</p> required Note <p>This is a no-op implementation that performs no actual task building.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>def build_task(self, uid, task_desc, task_specific_kwargs):\n    \"\"\"Build or prepare a task for execution.\n\n    Args:\n        uid: Unique identifier for the task.\n        task_desc: Dictionary containing task description and metadata.\n        task_specific_kwargs: Backend-specific keyword arguments for the task.\n\n    Note:\n        This is a no-op implementation that performs no actual task building.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend.submit_tasks","title":"submit_tasks  <code>async</code>","text":"<pre><code>submit_tasks(tasks)\n</code></pre> <p>Submit tasks for mock execution.</p> <p>Immediately marks all tasks as completed with dummy output without performing any actual computation.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <p>List of task dictionaries to be processed. Each task will receive dummy stdout and return_value before being marked as DONE.</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>async def submit_tasks(self, tasks):\n    \"\"\"Submit tasks for mock execution.\n\n    Immediately marks all tasks as completed with dummy output without\n    performing any actual computation.\n\n    Args:\n        tasks: List of task dictionaries to be processed. Each task will\n            receive dummy stdout and return_value before being marked as DONE.\n    \"\"\"\n    for task in tasks:\n        task[\"stdout\"] = \"Dummy Output\"\n        task[\"return_value\"] = \"Dummy Output\"\n        self._callback_func(task, \"DONE\")\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend.link_explicit_data_deps","title":"link_explicit_data_deps","text":"<pre><code>link_explicit_data_deps(src_task=None, dst_task=None, file_name=None, file_path=None)\n</code></pre> <p>Handle explicit data dependencies between tasks.</p> <p>Parameters:</p> Name Type Description Default <code>src_task</code> <p>The source task that produces the dependency.</p> <code>None</code> <code>dst_task</code> <p>The destination task that depends on the source.</p> <code>None</code> <code>file_name</code> <p>Name of the file that represents the dependency.</p> <code>None</code> <code>file_path</code> <p>Full path to the file that represents the dependency.</p> <code>None</code> Note <p>This is a no-op implementation as this backend doesn't handle dependencies.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>def link_explicit_data_deps(\n    self, src_task=None, dst_task=None, file_name=None, file_path=None\n):\n    \"\"\"Handle explicit data dependencies between tasks.\n\n    Args:\n        src_task: The source task that produces the dependency.\n        dst_task: The destination task that depends on the source.\n        file_name: Name of the file that represents the dependency.\n        file_path: Full path to the file that represents the dependency.\n\n    Note:\n        This is a no-op implementation as this backend doesn't handle dependencies.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend.link_implicit_data_deps","title":"link_implicit_data_deps","text":"<pre><code>link_implicit_data_deps(src_task, dst_task)\n</code></pre> <p>Handle implicit data dependencies for a task.</p> <p>Parameters:</p> Name Type Description Default <code>src_task</code> <p>The source task that produces data.</p> required <code>dst_task</code> <p>The destination task that depends on the source task's output.</p> required Note <p>This is a no-op implementation as this backend doesn't handle dependencies.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>def link_implicit_data_deps(self, src_task, dst_task):\n    \"\"\"Handle implicit data dependencies for a task.\n\n    Args:\n        src_task: The source task that produces data.\n        dst_task: The destination task that depends on the source task's output.\n\n    Note:\n        This is a no-op implementation as this backend doesn't handle dependencies.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/noop/#radical.asyncflow.backends.execution.noop.NoopExecutionBackend.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown the no-op execution backend.</p> <p>Performs cleanup operations. Since this is a no-op backend, no actual resources need to be cleaned up.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/noop.py</code> <pre><code>async def shutdown(self) -&gt; None:\n    \"\"\"Shutdown the no-op execution backend.\n\n    Performs cleanup operations. Since this is a no-op backend, no actual\n    resources need to be cleaned up.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/","title":"radical_pilot","text":""},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radicalasyncflowbackendsexecutionradical_pilot","title":"radical.asyncflow.backends.execution.radical_pilot","text":""},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend","title":"RadicalExecutionBackend","text":"<pre><code>RadicalExecutionBackend(resources: dict, raptor_config: Optional[dict] = None)\n</code></pre> <p>               Bases: <code>BaseExecutionBackend</code></p> <p>Radical Pilot-based execution backend for large-scale HPC task execution.</p> <p>The RadicalExecutionBackend manages computing resources and task execution using the Radical Pilot framework. It interfaces with various resource management systems (SLURM, FLUX, etc.) on diverse HPC machines, providing capabilities for session management, task lifecycle control, and resource allocation.</p> <p>This backend supports both traditional task execution and advanced features like Raptor mode for high-throughput computing scenarios. It handles pilot submission, task management, and provides data dependency linking mechanisms.</p> <p>Attributes:</p> Name Type Description <code>session</code> <code>Session</code> <p>Primary session for managing task execution context, uniquely identified by a generated ID.</p> <code>task_manager</code> <code>TaskManager</code> <p>Manages task lifecycle including submission, tracking, and completion within the session.</p> <code>pilot_manager</code> <code>PilotManager</code> <p>Coordinates computing resources (pilots) that are dynamically allocated based on resource requirements.</p> <code>resource_pilot</code> <code>Pilot</code> <p>Submitted computing resources configured according to the provided resource specifications.</p> <code>tasks</code> <code>dict</code> <p>Dictionary storing task descriptions indexed by UID.</p> <code>raptor_mode</code> <code>bool</code> <p>Flag indicating whether Raptor mode is enabled.</p> <code>masters</code> <code>list</code> <p>List of master tasks when Raptor mode is enabled.</p> <code>workers</code> <code>list</code> <p>List of worker tasks when Raptor mode is enabled.</p> <code>master_selector</code> <code>callable</code> <p>Generator for load balancing across masters.</p> <code>_callback_func</code> <code>Callable</code> <p>Registered callback function for task events.</p> <p>Parameters:</p> Name Type Description Default <code>resources</code> <code>dict</code> <p>Resource requirements for the pilot including CPU, GPU, and memory specifications.</p> required <code>raptor_config</code> <code>Optional[dict]</code> <p>Configuration for enabling Raptor mode. Contains master and worker task specifications.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If session creation, pilot submission, or task manager setup fails.</p> <code>SystemExit</code> <p>If KeyboardInterrupt or SystemExit occurs during initialization.</p> Example <p>::     resources = {         \"resource\": \"local.localhost\",         \"runtime\": 30,         \"exit_on_error\": True,         \"cores\": 4     }     backend = await RadicalExecutionBackend(resources)</p> <pre><code># With Raptor mode\nraptor_config = {\n    \"masters\": [{\n        \"executable\": \"/path/to/master\",\n        \"arguments\": [\"--config\", \"master.conf\"],\n        \"ranks\": 1,\n        \"workers\": [{\n            \"executable\": \"/path/to/worker\",\n            \"arguments\": [\"--mode\", \"compute\"],\n            \"ranks\": 4\n        }]\n    }]\n}\nbackend = await RadicalExecutionBackend(resources, raptor_config)\n</code></pre> <p>Initialize the RadicalExecutionBackend with resources.</p> <p>Creates a new Radical Pilot session, initializes task and pilot managers, submits pilots based on resource configuration, and optionally enables Raptor mode for high-throughput computing.</p> <p>Parameters:</p> Name Type Description Default <code>resources</code> <code>Dict</code> <p>Resource configuration for the Radical Pilot session. Must contain valid pilot description parameters such as: - resource: Target resource (e.g., \"local.localhost\") - runtime: Maximum runtime in minutes - cores: Number of CPU cores - gpus: Number of GPUs (optional)</p> required <code>raptor_config</code> <code>Optional[Dict]</code> <p>Configuration for Raptor mode containing: - masters: List of master task configurations - Each master can have associated workers Defaults to None (Raptor mode disabled).</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If RadicalPilot backend fails to initialize properly.</p> <code>SystemExit</code> <p>If keyboard interrupt or system exit occurs during setup, with session path information for debugging.</p> Note <ul> <li>Automatically registers backend states with the global StateMapper</li> <li>logs status messages for successful initialization or failures</li> <li>Session UID is generated using radical.utils for uniqueness</li> </ul> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>@typeguard.typechecked\ndef __init__(self, resources: dict, raptor_config: Optional[dict] = None) -&gt; None:\n    \"\"\"Initialize the RadicalExecutionBackend with resources.\n\n    Creates a new Radical Pilot session, initializes task and pilot managers,\n    submits pilots based on resource configuration, and optionally enables\n    Raptor mode for high-throughput computing.\n\n    Args:\n        resources (Dict): Resource configuration for the Radical Pilot session.\n            Must contain valid pilot description parameters such as:\n            - resource: Target resource (e.g., \"local.localhost\")\n            - runtime: Maximum runtime in minutes\n            - cores: Number of CPU cores\n            - gpus: Number of GPUs (optional)\n        raptor_config (Optional[Dict]): Configuration for Raptor mode containing:\n            - masters: List of master task configurations\n            - Each master can have associated workers\n            Defaults to None (Raptor mode disabled).\n\n    Raises:\n        Exception: If RadicalPilot backend fails to initialize properly.\n        SystemExit: If keyboard interrupt or system exit occurs during setup,\n            with session path information for debugging.\n\n    Note:\n        - Automatically registers backend states with the global StateMapper\n        - logs status messages for successful initialization or failures\n        - Session UID is generated using radical.utils for uniqueness\n    \"\"\"\n\n    if rp is None or ru is None:\n        raise ImportError(\n            \"Radical.Pilot and Radical.utils are required for \"\n            \"RadicalExecutionBackend.\"\n        )\n\n    self.resources = resources\n    self.raptor_config = raptor_config or {}\n    self._initialized = False\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.__await__","title":"__await__","text":"<pre><code>__await__()\n</code></pre> <p>Make RadicalExecutionBackend awaitable.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def __await__(self):\n    \"\"\"Make RadicalExecutionBackend awaitable.\"\"\"\n    return self._async_init().__await__()\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.get_task_states_map","title":"get_task_states_map","text":"<pre><code>get_task_states_map() -&gt; StateMapper\n</code></pre> <p>Get the state mapper for this backend.</p> <p>Returns:</p> Name Type Description <code>StateMapper</code> <code>StateMapper</code> <p>StateMapper instance configured for RadicalPilot backend with appropriate state mappings (DONE, FAILED, CANCELED, AGENT_EXECUTING).</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def get_task_states_map(self) -&gt; StateMapper:\n    \"\"\"Get the state mapper for this backend.\n\n    Returns:\n        StateMapper: StateMapper instance configured for RadicalPilot backend\n            with appropriate state mappings (DONE, FAILED,\n            CANCELED, AGENT_EXECUTING).\n    \"\"\"\n    return StateMapper(backend=self)\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.setup_raptor_mode","title":"setup_raptor_mode","text":"<pre><code>setup_raptor_mode(raptor_config: dict) -&gt; None\n</code></pre> <p>Set up Raptor mode by configuring and submitting master and worker tasks.</p> <p>Initializes Raptor mode by creating master tasks and their associated worker tasks based on the provided configuration. Masters coordinate work distribution while workers execute the actual computations.</p> <p>Parameters:</p> Name Type Description Default <code>raptor_config</code> <code>Dict</code> <p>Configuration dictionary with the following structure: {     'masters': [         {             'executable': str,  # Path to master executable             'arguments': list,  # Arguments for master             'ranks': int,       # Number of CPU processes             'workers': [        # Worker configurations                 {                     'executable': str,    # Worker executable path                     'arguments': list,    # Worker arguments                     'ranks': int,         # Worker CPU processes                     'worker_type': str    # Optional worker class                 },                 ...             ]         },         ...     ] }</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If task description creation or submission fails.</p> Note <ul> <li>Creates unique UIDs for masters and workers using session namespace</li> <li>Sets up master selector for load balancing across masters</li> <li>Workers default to 'DefaultWorker' class if not specified</li> <li>All master and worker tasks are stored in respective class attributes</li> </ul> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def setup_raptor_mode(self, raptor_config: dict) -&gt; None:\n    \"\"\"Set up Raptor mode by configuring and submitting master and worker tasks.\n\n    Initializes Raptor mode by creating master tasks and their associated\n    worker tasks based on the provided configuration. Masters coordinate\n    work distribution while workers execute the actual computations.\n\n    Args:\n        raptor_config (Dict): Configuration dictionary with the following structure:\n            {\n                'masters': [\n                    {\n                        'executable': str,  # Path to master executable\n                        'arguments': list,  # Arguments for master\n                        'ranks': int,       # Number of CPU processes\n                        'workers': [        # Worker configurations\n                            {\n                                'executable': str,    # Worker executable path\n                                'arguments': list,    # Worker arguments\n                                'ranks': int,         # Worker CPU processes\n                                'worker_type': str    # Optional worker class\n                            },\n                            ...\n                        ]\n                    },\n                    ...\n                ]\n            }\n\n    Raises:\n        Exception: If task description creation or submission fails.\n\n    Note:\n        - Creates unique UIDs for masters and workers using session namespace\n        - Sets up master selector for load balancing across masters\n        - Workers default to 'DefaultWorker' class if not specified\n        - All master and worker tasks are stored in respective class attributes\n    \"\"\"\n\n    self.masters = []\n    self.workers = []\n    self.master_selector = self.select_master()\n\n    cfg = copy.deepcopy(raptor_config)\n    masters = cfg[\"masters\"]\n\n    for master_description in masters:\n        workers = master_description.pop(\"workers\")\n        md = rp.TaskDescription(master_description)\n        md.uid = ru.generate_id(\n            \"flow.master.%(item_counter)06d\", ru.ID_CUSTOM, ns=self.session.uid\n        )\n        md.mode = rp.RAPTOR_MASTER\n        master = self.resource_pilot.submit_raptors(md)[0]\n        self.masters.append(master)\n\n        for worker_description in workers:\n            raptor_class = worker_description.pop(\"worker_type\", \"DefaultWorker\")\n            worker = master.submit_workers(\n                rp.TaskDescription(\n                    {\n                        **worker_description,\n                        \"raptor_id\": md.uid,\n                        \"mode\": rp.RAPTOR_WORKER,\n                        \"raptor_class\": raptor_class,\n                        \"uid\": ru.generate_id(\n                            \"flow.worker.%(item_counter)06d\",\n                            ru.ID_CUSTOM,\n                            ns=self.session.uid,\n                        ),\n                    }\n                )\n            )\n            self.workers.append(worker)\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.select_master","title":"select_master","text":"<pre><code>select_master()\n</code></pre> <p>Create a generator for load balancing task submission across masters.</p> <p>Provides a round-robin generator that cycles through available master UIDs to distribute tasks evenly across all masters in Raptor mode.</p> <p>Returns:</p> Type Description <p>Generator[str]: Generator yielding master UIDs in round-robin fashion.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If Raptor mode is not enabled or no masters are available.</p> Example <p>::     selector = backend.select_master()     master_uid = next(selector)  # Get next master for task assignment</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def select_master(self):\n    \"\"\"Create a generator for load balancing task submission across masters.\n\n    Provides a round-robin generator that cycles through available master\n    UIDs to distribute tasks evenly across all masters in Raptor mode.\n\n    Returns:\n        Generator[str]: Generator yielding master UIDs in round-robin fashion.\n\n    Raises:\n        RuntimeError: If Raptor mode is not enabled or no masters are available.\n\n    Example:\n        ::\n            selector = backend.select_master()\n            master_uid = next(selector)  # Get next master for task assignment\n    \"\"\"\n    if not self.raptor_mode or not self.masters:\n        raise RuntimeError(\"Raptor mode disabled or no masters available\")\n\n    current_master = 0\n    masters_uids = [m.uid for m in self.masters]\n\n    while True:\n        yield masters_uids[current_master]\n        current_master = (current_master + 1) % len(self.masters)\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.register_callback","title":"register_callback","text":"<pre><code>register_callback(func: Callable) -&gt; None\n</code></pre> <p>Register a callback function for task state changes.</p> <p>Sets up a callback mechanism that handles task state transitions, with special handling for service tasks that require additional readiness confirmation.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Callback function that will be invoked on task              state changes. Should accept parameters:              (task, state, service_callback=None).</p> required Note <ul> <li>Service tasks in AGENT_EXECUTING state get special service_ready_callback</li> <li>All other tasks use the standard callback mechanism</li> <li>The callback is registered with the underlying task manager</li> </ul> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def register_callback(self, func: Callable) -&gt; None:\n    \"\"\"Register a callback function for task state changes.\n\n    Sets up a callback mechanism that handles task state transitions,\n    with special handling for service tasks that require additional\n    readiness confirmation.\n\n    Args:\n        func (Callable): Callback function that will be invoked on task\n                         state changes. Should accept parameters:\n                         (task, state, service_callback=None).\n\n    Note:\n        - Service tasks in AGENT_EXECUTING state get special service_ready_callback\n        - All other tasks use the standard callback mechanism\n        - The callback is registered with the underlying task manager\n    \"\"\"\n    self._callback_func = func\n\n    def backend_callback(task, state) -&gt; None:\n        service_callback = None\n\n        if task.mode == rp.TASK_SERVICE and state == rp.AGENT_EXECUTING:\n            service_callback = service_ready_callback\n\n        elif task.mode == rp.TASK_EXECUTABLE and state == rp.FAILED:\n            task = task.as_dict()\n            stderr = task.get(\"stderr\")\n            exception = task.get(\"exception\")\n            if stderr or exception:\n                task[\"stderr\"] = \", \".join(filter(None, [stderr, exception]))\n                task[\"exception\"] = \"\"\n\n        func(task, state, service_callback=service_callback)\n\n    self.task_manager.register_callback(backend_callback)\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.build_task","title":"build_task","text":"<pre><code>build_task(uid: str, task_desc: dict, task_backend_specific_kwargs: dict) -&gt; Optional[TaskDescription]\n</code></pre> <p>Build a RadicalPilot task description from workflow task parameters.</p> <p>Converts a workflow task description into a RadicalPilot TaskDescription, handling different task modes (executable, function, service) and applying appropriate configurations.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Unique identifier for the task.</p> required <code>task_desc</code> <code>Dict</code> <p>Task description containing: - executable: Path to executable (for executable tasks) - function: Python function (for function tasks) - args: Function arguments - kwargs: Function keyword arguments - is_service: Boolean indicating service task</p> required <code>task_backend_specific_kwargs</code> <code>Dict</code> <p>RadicalPilot-specific parameters for the task description.</p> required <p>Returns:</p> Type Description <code>Optional[TaskDescription]</code> <p>rp.TaskDescription: Configured RadicalPilot task description, or None if task creation failed.</p> Note <ul> <li>Function tasks require Raptor mode to be enabled</li> <li>Service tasks cannot be Python functions</li> <li>Failed tasks trigger callback with FAILED state</li> <li>Raptor tasks are assigned to masters via load balancing</li> </ul> Example <p>::     task_desc = {         'executable': '/bin/echo',         'args': ['Hello World'],         'is_service': False     }     rp_task = backend.build_task('task_001', task_desc, {})</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def build_task(\n    self, uid: str, task_desc: dict, task_backend_specific_kwargs: dict\n) -&gt; \"Optional[rp.TaskDescription]\":\n    \"\"\"Build a RadicalPilot task description from workflow task parameters.\n\n    Converts a workflow task description into a RadicalPilot TaskDescription,\n    handling different task modes (executable, function, service) and applying\n    appropriate configurations.\n\n    Args:\n        uid (str): Unique identifier for the task.\n        task_desc (Dict): Task description containing:\n            - executable: Path to executable (for executable tasks)\n            - function: Python function (for function tasks)\n            - args: Function arguments\n            - kwargs: Function keyword arguments\n            - is_service: Boolean indicating service task\n        task_backend_specific_kwargs (Dict): RadicalPilot-specific parameters\n            for the task description.\n\n    Returns:\n        rp.TaskDescription: Configured RadicalPilot task description, or None\n            if task creation failed.\n\n    Note:\n        - Function tasks require Raptor mode to be enabled\n        - Service tasks cannot be Python functions\n        - Failed tasks trigger callback with FAILED state\n        - Raptor tasks are assigned to masters via load balancing\n\n    Example:\n        ::\n            task_desc = {\n                'executable': '/bin/echo',\n                'args': ['Hello World'],\n                'is_service': False\n            }\n            rp_task = backend.build_task('task_001', task_desc, {})\n    \"\"\"\n\n    is_service = task_desc.get(\"is_service\", False)\n    rp_task = rp.TaskDescription(from_dict=task_backend_specific_kwargs)\n    rp_task.uid = uid\n\n    if task_desc[\"executable\"]:\n        rp_task.mode = rp.TASK_SERVICE if is_service else rp.TASK_EXECUTABLE\n        rp_task.executable = task_desc[\"executable\"]\n    elif task_desc[\"function\"]:\n        if is_service:\n            error_msg = (\n                \"RadicalExecutionBackend does not support function service tasks\"\n            )\n            rp_task[\"exception\"] = ValueError(error_msg)\n            self._callback_func(rp_task, rp.FAILED)\n            return None\n\n        rp_task.mode = rp.TASK_FUNCTION\n        rp_task.function = rp.PythonTask(\n            task_desc[\"function\"], task_desc[\"args\"], task_desc[\"kwargs\"]\n        )\n\n    if rp_task.mode in [\n        rp.TASK_FUNCTION,\n        rp.TASK_EVAL,\n        rp.TASK_PROC,\n        rp.TASK_METHOD,\n    ]:\n        if not self.raptor_mode:\n            error_msg = f\"Raptor mode not enabled, cannot register {rp_task.mode}\"\n            rp_task[\"exception\"] = RuntimeError(error_msg)\n            self._callback_func(rp_task, rp.FAILED)\n            return None\n\n        rp_task.raptor_id = next(self.master_selector)\n\n    self.tasks[uid] = rp_task\n    return rp_task\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.link_explicit_data_deps","title":"link_explicit_data_deps","text":"<pre><code>link_explicit_data_deps(src_task: Optional[dict] = None, dst_task: Optional[dict] = None, file_name: Optional[str] = None, file_path: Optional[str] = None) -&gt; dict\n</code></pre> <p>Link explicit data dependencies between tasks or from external sources.</p> <p>Creates data staging entries to establish explicit dependencies where files are transferred or linked from source to destination tasks. Supports both task-to-task dependencies and external file staging.</p> <p>Parameters:</p> Name Type Description Default <code>src_task</code> <code>Optional[Dict]</code> <p>Source task dictionary containing the file. None when staging from external path.</p> <code>None</code> <code>dst_task</code> <code>Dict</code> <p>Destination task dictionary that will receive the file. Must contain 'task_backend_specific_kwargs' key.</p> <code>None</code> <code>file_name</code> <code>Optional[str]</code> <p>Name of the file to stage. Defaults to: - src_task UID if staging from task - basename of file_path if staging from external path</p> <code>None</code> <code>file_path</code> <code>Optional[str]</code> <p>External file path to stage (alternative to task-sourced files).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>dict</code> <p>The data dependency dictionary that was added to input staging.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither file_name nor file_path is provided, or if src_task is missing when file_path is not specified.</p> Note <ul> <li>External files use TRANSFER action</li> <li>Task-to-task dependencies use LINK action</li> <li>Files are staged to task:/// namespace in destination</li> <li>Input staging list is created if it doesn't exist</li> </ul> Example <p>::     # Link output from task1 to task2     backend.link_explicit_data_deps(         src_task={'uid': 'task1'},         dst_task={'task_backend_specific_kwargs': {}},         file_name='output.dat'     )</p> <pre><code># Stage external file\nbackend.link_explicit_data_deps(\n    dst_task={'task_backend_specific_kwargs': {}},\n    file_path='/path/to/input.txt'\n)\n</code></pre> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def link_explicit_data_deps(\n    self,\n    src_task: Optional[dict] = None,\n    dst_task: Optional[dict] = None,\n    file_name: Optional[str] = None,\n    file_path: Optional[str] = None,\n) -&gt; dict:\n    \"\"\"Link explicit data dependencies between tasks or from external sources.\n\n    Creates data staging entries to establish explicit dependencies where\n    files are transferred or linked from source to destination tasks.\n    Supports both task-to-task dependencies and external file staging.\n\n    Args:\n        src_task (Optional[Dict]): Source task dictionary containing the file.\n            None when staging from external path.\n        dst_task (Dict): Destination task dictionary that will receive the file.\n            Must contain 'task_backend_specific_kwargs' key.\n        file_name (Optional[str]): Name of the file to stage. Defaults to:\n            - src_task UID if staging from task\n            - basename of file_path if staging from external path\n        file_path (Optional[str]): External file path to stage (alternative\n            to task-sourced files).\n\n    Returns:\n        Dict: The data dependency dictionary that was added to input staging.\n\n    Raises:\n        ValueError: If neither file_name nor file_path is provided, or if\n            src_task is missing when file_path is not specified.\n\n    Note:\n        - External files use TRANSFER action\n        - Task-to-task dependencies use LINK action\n        - Files are staged to task:/// namespace in destination\n        - Input staging list is created if it doesn't exist\n\n    Example:\n        ::\n            # Link output from task1 to task2\n            backend.link_explicit_data_deps(\n                src_task={'uid': 'task1'},\n                dst_task={'task_backend_specific_kwargs': {}},\n                file_name='output.dat'\n            )\n\n            # Stage external file\n            backend.link_explicit_data_deps(\n                dst_task={'task_backend_specific_kwargs': {}},\n                file_path='/path/to/input.txt'\n            )\n    \"\"\"\n    if not file_name and not file_path:\n        raise ValueError(\"Either file_name or file_path must be provided\")\n\n    dst_kwargs = dst_task[\"task_backend_specific_kwargs\"]\n\n    if not file_name:\n        if file_path:\n            file_name = file_path.split(\"/\")[-1]\n        elif src_task:\n            file_name = src_task[\"uid\"]\n        else:\n            raise ValueError(\"Must provide file_name, file_path, or src_task\")\n\n    if file_path:\n        data_dep = {\n            \"source\": file_path,\n            \"target\": f\"task:///{file_name}\",\n            \"action\": rp.TRANSFER,\n        }\n    else:\n        if not src_task:\n            raise ValueError(\"src_task required when file_path not specified\")\n        data_dep = {\n            \"source\": f\"pilot:///{src_task['uid']}/{file_name}\",\n            \"target\": f\"task:///{file_name}\",\n            \"action\": rp.LINK,\n        }\n\n    if \"input_staging\" not in dst_kwargs:\n        dst_kwargs[\"input_staging\"] = [data_dep]\n    else:\n        dst_kwargs[\"input_staging\"].append(data_dep)\n\n    return data_dep\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.link_implicit_data_deps","title":"link_implicit_data_deps","text":"<pre><code>link_implicit_data_deps(src_task: dict, dst_task: dict) -&gt; None\n</code></pre> <p>Add implicit data dependencies through symbolic links in task sandboxes.</p> <p>Creates pre-execution commands that establish symbolic links from the source task's sandbox to the destination task's sandbox, simulating implicit data dependencies without explicit file specifications.</p> <p>Parameters:</p> Name Type Description Default <code>src_task</code> <code>Dict</code> <p>Source task dictionary containing 'uid' key.</p> required <code>dst_task</code> <code>Dict</code> <p>Destination task dictionary with</p> required Note <ul> <li>Links all files from source sandbox except the task UID file itself</li> <li>Uses environment variables for source task identification</li> <li>Commands are added to the destination task's pre_exec list</li> <li>Symbolic links are created in the destination task's sandbox</li> </ul> Implementation Details <ol> <li>Sets SRC_TASK_ID environment variable</li> <li>Sets SRC_TASK_SANDBOX path variable</li> <li>Creates symbolic links for all files except the task ID file</li> </ol> Example <p>::     src_task = {'uid': 'producer_task'}     dst_task = {'task_backend_specific_kwargs': {}}     backend.link_implicit_data_deps(src_task, dst_task)</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def link_implicit_data_deps(self, src_task: dict, dst_task: dict) -&gt; None:\n    \"\"\"Add implicit data dependencies through symbolic links in task sandboxes.\n\n    Creates pre-execution commands that establish symbolic links from the\n    source task's sandbox to the destination task's sandbox, simulating\n    implicit data dependencies without explicit file specifications.\n\n    Args:\n        src_task (Dict): Source task dictionary containing 'uid' key.\n        dst_task (Dict): Destination task dictionary with\n        'task_backend_specific_kwargs'.\n\n    Note:\n        - Links all files from source sandbox except the task UID file itself\n        - Uses environment variables for source task identification\n        - Commands are added to the destination task's pre_exec list\n        - Symbolic links are created in the destination task's sandbox\n\n    Implementation Details:\n        1. Sets SRC_TASK_ID environment variable\n        2. Sets SRC_TASK_SANDBOX path variable\n        3. Creates symbolic links for all files except the task ID file\n\n    Example:\n        ::\n            src_task = {'uid': 'producer_task'}\n            dst_task = {'task_backend_specific_kwargs': {}}\n            backend.link_implicit_data_deps(src_task, dst_task)\n    \"\"\"\n\n    dst_kwargs = dst_task[\"task_backend_specific_kwargs\"]\n    src_uid = src_task[\"uid\"]\n\n    cmd1 = f\"export SRC_TASK_ID={src_uid}\"\n    cmd2 = 'export SRC_TASK_SANDBOX=\"$RP_PILOT_SANDBOX/$SRC_TASK_ID\"'\n    cmd3 = \"\"\"files=$(cd \"$SRC_TASK_SANDBOX\" &amp;&amp; ls | grep -ve \"^$SRC_TASK_ID\")\n            for f in $files\n            do\n                ln -sf \"$SRC_TASK_SANDBOX/$f\" \"$RP_TASK_SANDBOX\"\n            done\"\"\"\n\n    commands = [cmd1, cmd2, cmd3]\n\n    if dst_kwargs.get(\"pre_exec\"):\n        dst_kwargs[\"pre_exec\"].extend(commands)\n    else:\n        dst_kwargs[\"pre_exec\"] = commands\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.submit_tasks","title":"submit_tasks  <code>async</code>","text":"<pre><code>submit_tasks(tasks: list) -&gt; None\n</code></pre> <p>Submit a list of tasks for execution.</p> <p>Processes a list of workflow tasks, builds RadicalPilot task descriptions, and submits them to the task manager for execution. Handles task building failures gracefully by skipping invalid tasks.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>list</code> <p>List of task dictionaries, each containing: - uid: Unique task identifier - task_backend_specific_kwargs: RadicalPilot-specific parameters - Other task description fields</p> required <p>Returns:</p> Type Description <code>None</code> <p>The result of task_manager.submit_tasks() with successfully built tasks.</p> Note <ul> <li>Failed task builds are skipped (build_task returns None)</li> <li>Only successfully built tasks are submitted to the task manager</li> <li>Task building includes validation and error handling</li> </ul> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>async def submit_tasks(self, tasks: list) -&gt; None:\n    \"\"\"Submit a list of tasks for execution.\n\n    Processes a list of workflow tasks, builds RadicalPilot task descriptions,\n    and submits them to the task manager for execution. Handles task building\n    failures gracefully by skipping invalid tasks.\n\n    Args:\n        tasks (list): List of task dictionaries, each containing:\n            - uid: Unique task identifier\n            - task_backend_specific_kwargs: RadicalPilot-specific parameters\n            - Other task description fields\n\n    Returns:\n        The result of task_manager.submit_tasks() with successfully built tasks.\n\n    Note:\n        - Failed task builds are skipped (build_task returns None)\n        - Only successfully built tasks are submitted to the task manager\n        - Task building includes validation and error handling\n    \"\"\"\n\n    _tasks = []\n    for task in tasks:\n        task_to_submit = self.build_task(\n            task[\"uid\"], task, task[\"task_backend_specific_kwargs\"]\n        )\n        if not task_to_submit:\n            continue\n        _tasks.append(task_to_submit)\n\n    return self.task_manager.submit_tasks(_tasks)\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.cancel_task","title":"cancel_task  <code>async</code>","text":"<pre><code>cancel_task(uid: str) -&gt; bool\n</code></pre> <p>Cancel a task.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Task UID to cancel.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if task found and cancellation attempted, False otherwise.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>async def cancel_task(self, uid: str) -&gt; bool:\n    \"\"\"Cancel a task.\n\n    Args:\n        uid: Task UID to cancel.\n\n    Returns:\n        True if task found and cancellation attempted, False otherwise.\n    \"\"\"\n    if uid in self.tasks:\n        self.task_manager.cancel_tasks(uid)\n        return True\n    return False\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.get_nodelist","title":"get_nodelist","text":"<pre><code>get_nodelist() -&gt; Optional[NodeList]\n</code></pre> <p>Get information about allocated compute nodes.</p> <p>Retrieves the nodelist from the active resource pilot, providing details about the compute nodes allocated for task execution.</p> <p>Returns:</p> Type Description <code>Optional[NodeList]</code> <p>rp.NodeList: NodeList object containing information about allocated nodes. Each node in nodelist.nodes is of type rp.NodeResource. Returns None if the pilot is not in PMGR_ACTIVE state.</p> Note <ul> <li>Only returns nodelist when pilot is in active state</li> <li>Nodelist provides detailed resource information for each node</li> <li>Useful for resource-aware task scheduling and monitoring</li> </ul> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def get_nodelist(self) -&gt; \"Optional[rp.NodeList]\":\n    \"\"\"Get information about allocated compute nodes.\n\n    Retrieves the nodelist from the active resource pilot, providing\n    details about the compute nodes allocated for task execution.\n\n    Returns:\n        rp.NodeList: NodeList object containing information about allocated\n            nodes. Each node in nodelist.nodes is of type rp.NodeResource.\n            Returns None if the pilot is not in PMGR_ACTIVE state.\n\n    Note:\n        - Only returns nodelist when pilot is in active state\n        - Nodelist provides detailed resource information for each node\n        - Useful for resource-aware task scheduling and monitoring\n    \"\"\"\n\n    nodelist = None\n    if self.resource_pilot.state == rp.PMGR_ACTIVE:\n        nodelist = self.resource_pilot.nodelist\n    return nodelist\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.state","title":"state","text":"<pre><code>state()\n</code></pre> <p>Retrieve resource pilot state.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def state(self):\n    \"\"\"Retrieve resource pilot state.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.task_state_cb","title":"task_state_cb","text":"<pre><code>task_state_cb(task, state) -&gt; None\n</code></pre> <p>Handle task state changes.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def task_state_cb(self, task, state) -&gt; None:\n    \"\"\"Handle task state changes.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Gracefully shutdown the backend and clean up resources.</p> <p>Closes the RadicalPilot session with data download, ensuring proper cleanup of all resources including pilots, tasks, and session data.</p> Note <ul> <li>Downloads session data before closing</li> <li>Ensures graceful termination of all backend resources</li> <li>Prints confirmation message when shutdown is triggered</li> </ul> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>async def shutdown(self) -&gt; None:\n    \"\"\"Gracefully shutdown the backend and clean up resources.\n\n    Closes the RadicalPilot session with data download, ensuring proper\n    cleanup of all resources including pilots, tasks, and session data.\n\n    Note:\n        - Downloads session data before closing\n        - Ensures graceful termination of all backend resources\n        - Prints confirmation message when shutdown is triggered\n    \"\"\"\n    self.session.close(download=True)\n    logger.info(\"Radical Pilot backend shutdown complete\")\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__()\n</code></pre> <p>Async context manager entry.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Async context manager entry.\"\"\"\n    if not self._initialized:\n        await self._async_init()\n    return self\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Async context manager exit.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Async context manager exit.\"\"\"\n    await self.shutdown()\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.RadicalExecutionBackend.create","title":"create  <code>async</code> <code>classmethod</code>","text":"<pre><code>create(resources: dict, raptor_config: Optional[dict] = None)\n</code></pre> <p>Create initialized backend.</p> <p>Parameters:</p> Name Type Description Default <code>resources</code> <code>dict</code> <p>Radical Pilot configuration.</p> required <code>raptor_config</code> <code>Optional[dict]</code> <p>Optional Raptor mode configuration.</p> <code>None</code> <p>Returns:</p> Type Description <p>Initialized RadicalExecutionBackend instance.</p> Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>@classmethod\nasync def create(cls, resources: dict, raptor_config: Optional[dict] = None):\n    \"\"\"Create initialized backend.\n\n    Args:\n        resources: Radical Pilot configuration.\n        raptor_config: Optional Raptor mode configuration.\n\n    Returns:\n        Initialized RadicalExecutionBackend instance.\n    \"\"\"\n    backend = cls(resources, raptor_config)\n    return await backend\n</code></pre>"},{"location":"api/radical/asyncflow/backends/execution/radical_pilot/#radical.asyncflow.backends.execution.radical_pilot.service_ready_callback","title":"service_ready_callback","text":"<pre><code>service_ready_callback(future: Future, task, state) -&gt; None\n</code></pre> <p>Callback for handling service task readiness.</p> <p>Runs wait_info() in a daemon thread to avoid blocking execution flow.</p> <p>Parameters:</p> Name Type Description Default <code>future</code> <code>Future</code> <p>Future object to set result or exception.</p> required <code>task</code> <p>Task with wait_info() method.</p> required <code>state</code> <p>Current task state (unused).</p> required Source code in <code>doc_env/lib/python3.13/site-packages/radical/asyncflow/backends/execution/radical_pilot.py</code> <pre><code>def service_ready_callback(future: asyncio.Future, task, state) -&gt; None:\n    \"\"\"Callback for handling service task readiness.\n\n    Runs wait_info() in a daemon thread to avoid blocking execution flow.\n\n    Args:\n        future: Future object to set result or exception.\n        task: Task with wait_info() method.\n        state: Current task state (unused).\n    \"\"\"\n\n    def wait_and_set() -&gt; None:\n        try:\n            info = task.wait_info()  # synchronous call\n            future.set_result(info)\n        except Exception as e:\n            future.set_exception(e)\n\n    threading.Thread(target=wait_and_set, daemon=True).start()\n</code></pre>"}]}